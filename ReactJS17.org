# -*- mode:org; -*-

#+title:Learning ReactJS17
#+subtitle:{{{version}}} {{{date}}}
#+author:LOLH
#+date:2020-12-22 22:53
#+macro:version Version 0.0.10
#+macro:upload-date (eval (current-time-string))
#+bucket:pinecone-forest.com

{{{version}}} {{{date}}}

#+texinfo:@insertcopying


* Readme
:PROPERTIES:
:unnumbered: t
:custom_id: README
:END:
** About This Document

 This Readme should be tangled and included in all git commits.	It’s purpose is
 to provide an introduction to viewers of the source code on GitHub.

 #+texinfo:@heading Using the ABSTRACT Entry

 Each Readme should also provide an =ABSTRACT= entry for use by the ~sync~
 command.  Each Org source file in a group of related ~.info~ blogs produces a
 directory of linked HTML files via the ~make html~ command (which runs
 ~makeinfo --html <project>.texi~).  The directory of linked HTML files is
 uploaded by the AWS ~make sync~ command into a separate directory of the chosen
 AWS bucket for this group of related ~.info~ blogs.  Given a bucket name
 ~project~, and an ~.info~ blog named ~all-about-something~, with a version
 number of ~v0.1.2~, the group of HTML files will be uploaded to an AWS bucket
 that can be accessed like so:
 : https://<project>.com/all-about-something-v0.1.2/

 At the same time, the code running the ~make sync~ command will copy the text
 from the =ABSTRACT= section of the Readme and create a directory at the domain
 level of the AWS bucket linking to the current version of the ~index.html~ of
 this subproject.  The =ABSTRACT= entry will provide some context in the
 directory at the domain level, allowing the viewer to peruse the list of
 ~.info~ blogs and choose one based on its context.  In other words, by opening
 the project domain at:
 : https://<project>.com
 the user will see a list of all related subprojects which link directly to the
 most recently uploaded version.

** On Writing Markdown

 - [[https://daringfireball.net/projects/markdown/][Markdown Home]]
   Daring Fireball by John Gruber 17 Dec 2004.

   Markdown is a text-to-HTML conversion tool for web writers.

   Thus, “Markdown” is two things:
   1. a plain text  formatting syntax; you can view the  Markdown source for the
      article          text         on          this         page          here:
      https://daringfireball.net/projects/markdown/index.text[fn:1].

      The overriding design goal for Markdown’s  formatting syntax is to make it
      as readable as possible.

   2. (2) a software tool,  written in Perl, that  converts the plain text
      formatting to HTML

      (click to download ==> [[https://daringfireball.net/projects/downloads/Markdown_1.0.1.zip][=Markdown.pl=]])[fn:2].

      By default, Markdown produces XHTML output for tags with empty elements:

      : <br />

 - [[https://spec.commonmark.org/0.29/][CommonMark Spec]]
   Current Version 0.29 (2019-04-06) by John MacFarlane

   John Gruber’s canonical description of Markdown’s syntax does not specify the
   syntax unambiguously. =Markdown.pl= was quite  buggy, and gave manifestly bad
   results in many cases,  so it was not a satisfactory  replacement for a spec.
   Because  there   is  no  unambiguous  spec,   implementations  have  diverged
   considerably. As a result, users are  often surprised to find that a document
   that renders one  way on one system (say, a  GitHub wiki) renders differently
   on another (say, converting to docbook using pandoc).

   The   CommonMark   specification   attempts  to   specify   Markdown   syntax
   unambiguously. The examples  are intended to double as  conformance tests. An
   accompanying script  spec_tests.py can be used  to run the tests  against any
   Markdown program:

   : python test/spec_tests.py --spec spec.txt --program PROGRAM

 - [[https://github.github.com/gfm/][GitHub Flavored Markdown Spec]]

   GitHub Flavored Markdown, often shortened as  GFM, is the dialect of Markdown
   that  is  currently supported  for  user  content  on GitHub.com  and  GitHub
   Enterprise.

   GitHub-flavored  Markdown  is   based  upon  the  CommonMark   Spec  by  John
   MacFarlane. This formal specification, based  on the CommonMark Spec, defines
   the  syntax and  semantics  of this  dialect.  GFM is  a  strict superset  of
   CommonMark. All the  features which are supported in GitHub  user content and
   that are  not specified on  the original CommonMark  Spec are hence  known as
   extensions, and highlighted as such.

   Some  features  of  GitHub  Flavored  Markdown  are  only  available  in  the
   descriptions  and  comments  of  Issues  and  Pull  Requests.  These  include
   =@mentions= as well as references to SHA-1 hashes, Issues, and Pull Requests.
   Task Lists are also available in Gist comments and in Gist Markdown files.

   - Some Extensions

     - Syntax highlighting

     - Task Lists

     - Tables

     - SHA references

     - Issue references within a repository

     - Username =@mentions=

     - Automatic linking for URLs

     - Strikethrough

     - Emoji

 - [[https://guides.github.com/features/mastering-markdown/][Mastering Markdown]]

   This is a GitHub Guide, fairly simple in nature.

 - [[https://help.github.com/en/github/writing-on-github][Writing on GitHub]]

   This is also a GitHub Guide, more extensive and advanced in nature than
   Mastering Markdown.

 - [[https://guides.github.com/pdfs/markdown-cheatsheet-online.pdf][Markdown Syntax]]

   This is a GitHub Cheatsheet-style guide.

 - [[https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet][Markdown Cheatsheet]]

   This is intended as a quick reference and showcase.

 - [[https://docs.github.com/en/rest/reference/markdown][GitHub REST API v3 --- Markdown]]

** On Org and Org Babel
- [[https://orgmode.org][Org Mode]]

  Org-mode is  a powerful system for  organizing your complex life  with simple
  plain-text files.  It seamlessly  integrates all  your
  - notes,
  - mindmaps,
  - TODO lists,
  - calendar,
  - day planner, and
  - project schedules


  into a  single system that can  be easily
  - searched (e.g.  by grep),
  - encrypted (e.g. by GnuPG),
  - backed up and synced (e.g.  by Dropbox), and
  - imported/exported, (e.g. on  an iPhone  or Android  smartphone).


  It can even be used for authoring web pages and documents.


- [[https://orgmode.org/guide/index.html][Org Mode Compact Guide]]
- [[https://orgmode.org/worg/orgcard.html][Org Reference Card]]
- [[https://orgmode.org/worg/org-configs/index.html][Org Mode Configuration and Customization Guide]]
  - =M-x org-customize=
- Org Mode Indices
  - [[https://orgmode.org/manual/Main-Index.html#Main-Index][Main Index]]
  - [[https://orgmode.org/manual/Key-Index.html#Key-Index][Key Index]]
  - [[https://orgmode.org/manual/Variable-Index.html#Variable-Index][Variable Index]]
- Org Help
  - ~apropos~ :: =[C-u] M-x apropos <pattern>=
  - ~apropos-command~ :: =[C-u] C-h a <word>=
  - ~describe-variable~ :: =C-h v <word>=
  - ~describe-function~ :: =C-h f <word>=
- [[https://code.orgmode.org/bzg/org-mode][Org Code on GitHub]]
- [[https://orgmode.org/worg/org-contrib/babel/index.html][Org Babel]]
- [[https://org-babel.readthedocs.io/en/latest/][Org Babel Reference Card]]
- [[https://orgmode.org/worg/dev/org-syntax.html][Org Syntax]]
- [[https://orgmode.org/worg/dev/org-element-api.html][Org Element API]]
- [[https://orgmode.org/worg/doc.html][Org Hooks---Commands---Options]]
- [[https://orgmode.org/worg/org-gtd-etc.html][GTD using Org]]
  Pomodoro using Org I always start a clock  when I work on a task. And for the
  Pomodoro technique I use also the org-timer module with some configuration.

  Activate the org-timer module:
  ---------

    (add-to-list 'org-modules 'org-timer)

  Set a default value for the timer, for example:
  -----

    (setq org-timer-default-timer 25)

  Modify the  org-clock-in so that  a timer is  started with the  default value
  except if a timer is already started:
  -----

    (add-hook 'org-clock-in-hook (lambda ()
      (if (not org-timer-current-timer)
      (org-timer-set-timer '(16)))))

* Introduction
:PROPERTIES:
:unnumbered: t
:END:
** TODO Ideas on how to make this a better system
   :PROPERTIES:
   :custom_id: todos
   :END:
*** TODO Install major dependencies
[2020-02-14 Fri 10:12]
- [ ] like ~aws2~
- [ ] Org-mode > 9.1.9

*** TODO Get rid of hard-coded directories
- [ ] ~create-script~
  [2020-02-14 Fri 09:45]

  This script places itself inside a ~bin/~ directory so it can be found by
  searching $PATH.  This can be accomplished by creating an environment
  variable; I need to come up with a good domain name for this system first.
  How about =SyncOrg=?	So the environment variable would be =SYNC_ORG_BIN=.  A
  possible value would =~/Dev/bin= or =/usr/local/dev/bin=, depending on the
  system.

*** TODO Add check for existence of environment variables
[2020-02-14 Fri 09:57]
- [ ] it appears environment variables do not expand inside header lines? Check
  this.
- [ ] if environment variables do not expand, then need to provide a script to
  update them upon installation
- [ ] such as =SYNC_ORG_BIN=

*** DONE Update Date

- State "DONE"	     from "TODO"       [2020-03-02 Mon 21:12]
- State "TODO"	     from	       [2020-03-01 Sun 10:36] \\
  When creating a new project from this template, update the #+date: to
  the current day and time instead of leaving it to when this template
  was last updated.

*** TODO Make Update independent of the project

- State "TODO" from [2020-03-01 Sun 14:07] \\
  I just tried to  update a project, but the command AWS :=:  aws2 did not work
  because it  has changed  to aws  version 2.  If the  update program  had been
  independent and based  upon Template's most recent code, then  that would not
  have happened. Who knows what the  next problem will be causing the project's
  out-of-date code to fail to update.

**** TODO Another Update Problem
     - State "TODO" from [2020-10-23 Fri 06:52] \\
       I found another problem when trying to update. On a different computer I
       added the directory ~resources/~, placed inside it the directory ~png/~,
       and also  moved the  directory ~tools/~  into it.  I updated  the =Boot=
       mechanism,  but  failed to  add  the  new  =Boot=  Makefile to  the  Git
       repository. Thus, when I tried to update ~Template.org~, the out-of-date
       version of  ~Makefile~ installed and  ran, and  it pointed to  the wrong
       location for ~preprocess.el~.
       
       I need to find  a way to always save the =Boot=  Makefile in the right
       situation.  This would also apply to the ~README.md~ file.
*** DONE Make adding git repo an option
- State "DONE"	     from	       [2020-03-02 Mon 21:13]
*** TODO Default macros
    - State "TODO"       from              [2020-04-01 Wed 10:49] \\
      Default macros do not get expanded or in some cases even recognized.
      Is this a bug or am I doing something wrong.  User-defined macros
      work.
*** TODO Dependencies
:LOGBOOK:
- State "TODO"       from              [2020-06-09 Tue 23:36] \\
  Provide function to collect all dependencies and list; or perhaps test
:END:
*** TODO Add link to main index.html file
:LOGBOOK:
- State "TODO"       from              [2020-06-17 Wed 12:30] \\
  Want to create a link from project index.html files to the root
  index.html files.
:END:
*** TODO Make sure exec-path has correct values before proceeding
:LOGBOOK:
- State "TODO"       from              [2020-09-09 Wed 12:54] \\
  Make Sure exec-path has correct values
:END:
*** DONE Error on running initial 'make open-org'
   - State "DONE"       from "TODO"       [2020-09-08 Tue 23:10]
:LOGBOOK:
- State "TODO"       from              [2020-09-03 Thu 09:30] \\
  Fix error when running initial make from new project

  A: Made sure the default bucket was named correctly.
:END:
#+begin_src sh
> make open-org

  An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist
#+end_src
*** SOMEDAY Delete a whole project
:LOGBOOK:
- State "SOMEDAY"    from "TODO"       [2020-09-04 Fri 13:54] \\
  Provide functionality to remove a project from the file system,
  GitHub, and the AWS S3 bucket.
:END:
*** TODO Added Template Version when Updating
:LOGBOOK:
- State "TODO"       from              [2020-09-16 Wed 09:18] \\
  Add a custom id to an org template file when it is updated with the
  new template.
:END:
*** INPROGRESS Add a Missing Bucket on Update
    - State "INPROGRESS" from [2020-09-18 Fri 08:24] \\
      When updating, add a missing #+bucket: key by asking with a default of
      pinecone-forest.
*** TODO Check for Newer Version On Open Org
    - State "TODO"       from              [2020-09-19 Sat 10:09] \\
      When running the make command 'open-org', check for a newer version
      stored on GitHub and issue a warning if there is one.
*** DONE Revert INFO buffer upon new INFO file being generated
CLOSED: [2020-10-12 Mon 14:12]
:LOGBOOK:
- State "DONE"       from "INPROGRESS" [2020-10-12 Mon 14:12]
:END:
    - State "INPROGRESS" from "TODO"       [2020-09-21 Mon 07:56] \\
      Starting work on this TODO.
    - State "TODO"       from              [2020-09-21 Mon 07:54] \\
      After an INFO file is opened in Emacs as an INFO program, and a change is made to the ORG file, and a
      new INFO file is generated, the old INFO buffer is simply opened again; it must be manually reverted.
      Have the Makefile revert it if it exists instead.
*** TODO Add resources and images to AWS S3 bucket
:LOGBOOK:
- State "TODO"       from              [2020-10-12 Mon 14:12] \\
  Find a way to display images when syncing to the AWS S3 bucket.  Now
  they simply do not exist.
:END:
*** TODO Refactor ~org-template.el~ to use ~org-collect-keywords~ procedure
:LOGBOOK:
- State "TODO"       from              [2020-11-10 Tue 12:24] \\
  Just found procedure ~org-collect-keywords~; refactor to use it.
:END:
* ReactJS17
  #+cindex:ReactJS17
- [[https://reactjs.org][ReactJS17 Home]]
- [[https://reactjs.org/docs/getting-started.html][ReactJS17 Docs]]
- [[https://reactjs.org/tutorial/tutorial.html][ReactJS17 Tutorial]]
- [[https://reactjs.org/blog/2020/10/20/react-v17.html][ReactJS17 Blog]]
- [[https://reactjs.org/versions][React Versions]]
- [[https://github.com/facebook/react/releases][React on GitHub]]

** Adding React to a Web Page
 - https://reactjs.org/docs/add-react-to-a-website.html#add-react-in-one-minute

 #+cindex:React, add to web page
   1. Add a DOM  container anywhere inside the =<body>= of a  web site and give
      it a unique  =id= attribute. This allows React to  find the container and
      display a React component inside it. You may have as many independent DOM
      containers on  one page  as you  need. They should  be empty;  React will
      place content inside of them.

      #+begin_src html
	<!-- ... existing HTML ... -->

	<div id="like_button_container"></div>

	<!-- ... existing HTML ... -->
      #+end_src

   2. Add two =<script>=  tags for React and ReactDOM. The  third one will load
      your component code.

      #+cindex:React script tags
      #+begin_src html
	<body>
	  <!-- ... other HTML ... -->

	  <!-- Load React. -->
	  <!-- Note: when deploying, replace "development.js" with "production.min.js". -->
	  <script src="https://unpkg.com/react@17/umd/react.development.js" crossorigin></script>
	  <script src="https://unpkg.com/react-dom@17/umd/react-dom.development.js" crossorigin></script>

	  <!-- Load our React component. -->
	  <script src="like_button.js"></script>

	</body>
   #+end_src

   3. Create a React component in the file ~like_button.js~.

      #+cindex:React component, create
      This code defines a React component called =LikeButton=.
      #+begin_src js
	'use strict';

	const e = React.createElement;

	class LikeButton extends React.Component {
	  constructor(props) {
	    super(props);
	    this.state = { liked: false };
	  }

	  render() {
	    if (this.state.liked) {
	      return 'You liked this.';
	    }

	    return e(
	      'button',
	      { onClick: () => this.setState({ liked: true }) },
	      'Like'
	    );
	  }
	}
      #+end_src

      #+cindex:React component, mount
   4. Mount  the component in  the =<div>= =like_button_container=.  After the
      starter code, add two lines to  the bottom of ~like_button.js~. These two
      lines of code find the <div> we added  to our HTML in the first step, and
      then display our “Like” button React component inside of it.

      #+begin_src js
     // ... the starter code you pasted ...

     const domContainer = document.querySelector('#like_button_container');
     ReactDOM.render(e(LikeButton), domContainer);
   #+end_src

      You have just added the first React component to your website.

      #+cindex:React production script
      #+cindex:minify script
   5.  Minify  JavaScript for  production.  Before  deploying your  website  to
      production, be mindful that  unminified JavaScript can significantly slow
      down  the page  for  your users.  Use the  following  script sources  for
      production code:

      #+begin_src html
     <script src="https://unpkg.com/react@17/umd/react.development.js" crossorigin></script>
     <script src="https://unpkg.com/react-dom@17/umd/react-dom.development.js" crossorigin></script>
   #+end_src

   6. Add  JSX. The  quickest way to  try JSX  in your project  is to  add this
      =<script>= tag to your page:

      #+cindex:JSX
      #+begin_src html
	<script src="https://unpkg.com/babel-standalone@6/babel.min.js"></script>
      #+end_src

      Now you  can use  JSX in  any =<script>=  tag by  adding ~type="text/babel"~
      attribute to it. Change the "Like" button code as follows:

      #+begin_src js
	// const e = React.createElement;

	// // Display a "Like" <button>
	// return e(
	//   'button',
	//   { onClick: () => this.setState({ liked: true }) },
	//   'Like'
	// );

	// Display a "Like" <button>
	return (
	    <button onClick={() => this.setState({ liked: true })}>
	      Like
	    </button>
	);
   #+end_src

      These two code snippets are equivalent. JSX is completely optional.

** Add JSX and ES2015+ Syntax to a Project Using Babel

#+cindex:JSX, add to project
Adding JSX to a  project doesn’t require complicated tools like  a bundler or a
development  server.  Essentially, adding  JSX  is  a  lot  like adding  a  CSS
preprocessor.  The  only requirement  is  to  have  Node.js installed  on  your
computer.

#+cindex:Babel, add to project
Go to your project folder in the terminal, and paste these two commands:

1. Run ~npm init -y~
2. Run ~npm install babel-cli@6 babel-preset-react-app@3~

Both  React and  the  application code  can  stay as  =<script>=  tags with  no
changes. You just added a production-ready JSX setup to your project.

#+cindex:JSX preprocessor
3. Run  JSX Preprocessor. Create  a folder called  ~src~ and run  this terminal
   command, which starts an automated watcher for JSX.

   : npx babel --watch src --out-dir . --presets react-app/prod

   ~npx~ is a package runner tool that  comes with ~npm 5.2+~.

   See this [[https://medium.com/@maybekatz/introducing-npx-an-npm-package-runner-55f7d4bd282b][article describing npx]].

4. Now  create a file called  ~src/like_button.js~ with this JSX  starter code,
   the  watcher will  create  a preprocessed  ~like_button.js~ file  containing
   plain JavaScript  code suitable for  the browser.  When you edit  the source
   file with JSX, the transform will re-run automatically.

5. As a  bonus, this also lets  you use modern JavaScript  syntax features like
   classes without  worrying about  breaking older browsers.  The tool  we just
   used  is  called  =Babel=,  and  you  can  learn  more  about  it  from  its
   [[https://babeljs.io/docs/en/babel-cli/][documentation]].

#+begin_src js
  'use strict';

  class LikeButton extends React.Component {
    constructor(props) {
      super(props);
      this.state = { liked: false };
    }

    render() {
      if (this.state.liked) {
	return 'You liked this.';
      }

      return (
	<button onClick={() => this.setState({ liked: true }) }>
	  Like
	</button>
      );
    }
  }

  let domContainer = document.querySelector('#like_button_container');
  ReactDOM.render(<LikeButton />, domContainer);
#+end_src


* Pro React
  - [[https://github.com/Apress/pro-react][Pro React GitHub Files]]
  - [[file:resources/pro-react-master][pro-react-master directory]]

** Chapter One---Getting Started
*** Building Your First React App

  #+cindex:component
- React Component ::

  At the bare  minimum, a React component  is simply a JavaScript  class with a
  ~render~ method that returns a description of the component’s UI:

  #+begin_src js
    class Hello extends React.Component {
	render() {
	    return (
	      <h1>Hello World</h1>
	    )
	}
    }
  #+end_src

  #+cindex:JSX
- JSX ::

  You probably noticed the  HTML tags in the middle of  the JavaScript code. As
  mentioned, React has a syntax extension to JavaScript called JSX that lets us
  write XML  (and consequently HTML) inline  with code. JSX is  optional but it
  has  been widely  accepted  as the  standard  way of  defining  UIs in  React
  components because  of its declarative  syntax, expressiveness, and  the fact
  that it  gets converted  to plain  JavaScript function  calls, means  that it
  doesn’t alter the language semantics.

  #+cindex:transformation, JSX
  #+cindex:transpilation
  The important thing to consider now is that React requires a “transformation”
  step  (or  transpilation,  if  you  will) where  JSX  gets  transformed  into
  JavaScript.

*** React Development Workflow
In even the most basic scenarios, we  want a development workflow that allow us
to do the following:

 - Write JSX and transform it into regular JavaScript on the fly
 - Write code in a module pattern
 - Manage dependencies
 - Bundle JavaScript files and use source maps for debugging

With this in mind, the basic project structure for a React project contains the
following:

 1. A *source folder*, to contain all your JavaScript modules.

 2. An  ~index.html~ file.

    In  React applications,  the HTML  page  tends to  be almost  empty. It  is
    responsible only for  loading the application’s JavaScript  and providing a
    =div= (or any other element, actually) that  is used by React to render the
    application’s components into.

 3. A ~package.json~ file.

    The ~package.json~ is  a standard ~npm~ =manifest= file  that holds various
    information about the project, such  a
    - name,
    - description,
    - information about the author, etc.
    - It lets  the developer specify  dependencies (that can  get automatically
      downloaded and installed) and define script tasks.

    #+cindex:module, packager, build tool
 4. A  *module  packager*  or  *build  tool*,  which  will  be  used  for  JSX
    transformation and module/dependency bundling.

    The usage of *modules* helps organize  JavaScript code by splitting it into
    multiple  files,  each one  declaring  its  own dependencies.  The  *module
    bundler* then automatically  packs everything together in  the correct load
    order.

    #+cindex:webpack
    There are  a lot  of tools  that handle  this intermediary  step, including
     - ~Grunt~,
     - ~Gulp~,
     - ~Brunch~,
     - ~Webpack~

    among others. You can easily find recipes  for React in any of those tools,
    but in general, the React community  has adopted ~webpack~ as the preferred
    tool for this job. At its core, ~webpack~ is a *module bundler*, but it can
    also  put  the source  code  through  *loaders*  that can  /transform/  and
    /compile/ it.


Here is an outline of a minimum project files and folders structure:

- Project
  - ~source/~
    - ~App.js~
  - ~index.html~
  - ~package.json~
  - ~webpack.config.js~

**** React App Boilerplate
To keep focus  on learning the React  library, a React app  boilerplate pack is
provided with this book. The boilerplate project comes with all the basic files
and configurations  needed to  start developing immediately.

After downloading it,  all you have to  do is
- install the  dependencies and
- run the development  server to test  the project  in the browser.
- To automatically install all the  dependencies, open the terminal or command
  prompt and run
  : npm install
- To run the development server, simply type
  : npm start.


***** Objective
This boilerplate  is purposefully simple  to show  the minimal setup  needed to
create React projects  with Webpack and Babel.  It aims to be  a starting point
for learning  React, with  low cognitive  load and as  such avoids  having many
separate config  files and  advanced configuration  options, while  providing a
solid foundation for new React projects.

***** Usage
1. Create a clone of the source code
   : git clone git@github.com:pro-react/react-app-boilerplate.git
2. Enter the boilerplate source directory
   : cd react-app-boilerplate
3. Install its dependencies
   : npm install
4. Start the application in development mode
   : npm start
5. Open http://localhost:8080 in your browser.
   - =Static= files are  served from the ~public~ folder,
   - /project/ JavaScript files are bundled from the ~app~ folder.
6. Build for production
   : npm run build
   - This will generate a minimized ~bundle.js~ file on the ~public~ folder.

***** Missing Features
This  boilerplate  focuses  solely  on  transforming  and  bundling  javascript
files---all  other static  files are  served  directly from  the public  folder
without any processing.

While useful  for learning React, this  setup doesn't use Webpack  and Babel in
their  full capabilities---which  include
- transforming  and bundling  projects assets (such as stylesheets),
- modularizing CSS,
- hot reloading and etc.

***** Dependencies
- React & React-DOM
- Webpack & webpack-dev-server
- Babel Core
- Babel Loader (With "es2015" and "react" presets)

**** Advanced Boilerplate
- [[https://github.com/pro-react/react-app-advanced-boilerplate][Advanced Boilerplate on GitHub]]

"React project template with advanced Webpack setup."

***** Objective
While the  original Pro React  book App  Boilerplate is purposefully  simple to
show the minimal setup needed to  create React projects with Webpack and Babel,
this  is a  more complete  setup.  It is  based  on the  Appendix A  (entirelly
dedicated  to Webpack)  and features
- JavaScript and  CSS bundling,
- Hot Module Replacement,
- automatic HTML generation  and
- a separate production configuration with optimization and caching.

***** Usage
1. Clone this repository
   : git clone git@github.com:pro-react/react-app-advanced-boilerplate.git
2. Install
   : npm install
3. Start the application in development mode
   : npm start
   - Open http://localhost:8080 in your browser.
   - Static files are served from the ~public~ folder,
   - project JavaScript files are bundled from the ~app~ folder.
4. Build for production
   : npm run build
   - This will generate a minimized ~bundle.js~ file in the ~build~ folder.

**** Facebook's Create React App
Facebook released a  tool to create React Apps with  no need for configurations
or boilerplates. If you're just begining to  learn React, you might want to use
~create-react-app~  instead  of  using  this boilerplate  project.  Behind  the
scenes, create-react-app uses Webpack and Babel.

You might still want to use this boilerplate if you want to see how Webpack and
Babel can be manually configured in a project.
* React Development Tools
** Node Packages

#+cindex:package
- package ::

  A  *package* is  a  /folder tree/  described by  a  ~package.json~ file.  The
  *package* consists of  the folder containing the ~package.json~  file and all
  subfolders until the next folder containing another ~package.json~ file, or a
  folder named ~node_modules~.

  Package authors should  include the "type" field, even in  packages where all
  sources are  =CommonJS=. Being explicit  about the  type of the  package will
  future-proof the package in case the  default type of ~Node.j~s ever changes,
  and it will also make things easier  for build tools and loaders to determine
  how the files in the package should be interpreted.

- ES Modules ::

  ~Node.js~ will treat  the following as *ES modules* when  passed to ~node~ as
  the initial input, or when referenced by ~import~ statements within ES module
  code:

  - Files ending in ~.mjs~.

  - Files ending in ~.js~ when  the nearest parent ~package.json~ file contains
    a top-level "type" field with a value of "module".

  - Strings passed in as  an argument to ~--eval~, or piped  to node via =STDIN=,
    with the flag ~--input-type=module~.

- CommonJS Modules ::

  ~Node.js~ will treat as =CommonJS= all other forms of input. This behavior is
  to preserve backward compatibility. However, now that ~Node.js~ supports both
  =CommonJS= and  =ES= modules, it  is best  to be explicit  whenever possible.
  ~Node.js~ will treat the following as =CommonJS= when passed to ~node~ as the
  initial input,  or when  referenced by ~import~  statements within  ES module
  code:

  - Files ending in ~.cjs~.

  - Files ending in ~.js~ when  the nearest parent ~package.json~ file contains
    a top-level field "type" with a value of "commonjs".

  - files ending with ~.js~ files  where the nearest parent ~package.json~ file
    contains no top-level "type" field,

  - Strings passed  in as  an argument  to ~--eval~ or  ~--print~, or  piped to
    ~node~ via =STDIN=, with the flag ~--input-type=commonjs~.

  - string input without the flag ~--input-type~
*** Package Entry Points
In a  package’s ~package.json~ file, two  fields can define entry  points for a
package:

 - "main" :: The "main" field is  supported in all versions of ~Node.js~; thus,
   it can be  used as a fallback  for legacy versions of ~Node.js~  that do not
   support the "exports" field.

   To set the =main= entry point for  a package, it is advisable to define both
   "exports" and "main" in the package’s ~package.json~ file:

   #+begin_src js
     {
       "main": "./main.js",
       "exports": "./main.js"
     }
   #+end_src

   When  the "exports"  field  is  defined, all  subpaths  of  the package  are
   encapsulated and  no longer  available to  importers. This  encapsulation of
   exports provides more reliable guarantees about package interfaces for tools
   and  when  handling semver  upgrades  for  a package.  It  is  not a  strong
   encapsulation since a direct require of  any absolute subpath of the package
   such  as ~require('/path/to/node_modules/pkg/subpath.js')~  will still  load
   ~subpath.js~.

 - "exports" ::

   The "exports" field provides an alternative to "main" where the package main
   entry point can be defined  while also encapsulating the package, preventing
   any  other   entry  points   besides  those   defined  in   "exports".  This
   encapsulation allows module  authors to define a public  interface for their
   package. If both "exports" and "main" are defined, the "exports" field takes
   precedence over  "main". It is  best to  explicitly specify entry  points so
   that the package’s public API is well-defined.

 - subpath exports ::

   When using  the "exports" field, custom  subpaths can be defined  along with
   the  =main= entry  point  by treating  the  =main= entry  point  as the  "."
   subpath:

   #+begin_src js
     {
       "main": "./main.js",
       "exports": {
	 ".": "./main.js",
	 "./submodule": "./src/submodule.js"
       }
     }
   #+end_src

   Now only the defined subpath in "exports" can be imported by a consumer:

   : import submodule from 'es-module-package/submodule';
   : // Loads ./node_modules/es-module-package/src/submodule.js

   While other subpaths will error.
*** Node.js package.json field definitions
This section  describes the fields used  by the ~Node.js~ runtime.  Other tools
(such as  ~npm~) use additional fields  which are ignored by  ~Node.js~ and not
documented here.

 - =name= :: (string)

   Relevant when  using named imports  within a  package. Also used  by package
   managers as the name of the package.

 - =main= :: (string)

   The default  module when loading the  package, if exports is  not specified,
   and  in versions  of ~Node.js~  prior to  the introduction  of exports.  The
   "main" field defines  the script that is used when  the package directory is
   loaded via ~require()~. Its value is a path.

   : require('./path/to/directory'); // This resolves to ./path/to/directory/main.js.

 - =type= :: (string)

   The package  type determining whether to  load ~.js~ files as  =CommonJS= or
   =ES= modules. The "type" field defines the module format that ~Node.js~ uses
   for all  ~.js~ files  that have  that ~package.json~  file as  their nearest
   parent.

   The /nearest parent  ~package.json~/ is defined as  the first ~package.json~
   found when searching in the current  folder, that folder’s parent, and so on
   up until a ~node_modules~ folder or the volume root is reached.

 - =exports= :: (Object | string | string[])

   Package  exports  and  conditional   exports.  When  present,  limits  which
   submodules can be loaded from within the package. The "exports" field allows
   defining the entry  points of a package when imported  by name loaded either
   via a ~node_modules~ lookup or a self-reference to its own name.

 - =imports= :: (Object)

   Package imports, for use by modules within the package itself.
*** Node.js Modules
** NPM Packages
- [[https://docs.npmjs.com/packages-and-modules][NPM Packages Documentation]]

- package ::

  A *package* is a file or directory that is described by a ~package.json~ file.

  1) A folder containing a program described by a ~package.json~ file.

  2) A gzipped tarball containing (1).

  3) A URL that resolves to (2).

  4) A =<name>@<version>= that is published on the registry with (3).

  5) A =<name>@<tag>= that points to (4).

  6) A =<name>= that has a latest tag satisfying (5).

  7) A ~git~ url that, when cloned, results in (1).

     ~Git~ URLs used for ~npm~ packages can be formatted in the following ways:

     : git://github.com/user/project.git#commit-ish

     : git+ssh://user@hostname:project.git#commit-ish

     : git+http://user@hostname/project/blah.git#commit-ish

     : git+https://user@hostname/project/blah.git#commit-ish

- module ::

  A *module* is any file or  directory in the ~node_modules~ directory that can
  be loaded by the ~Node.js~ ~require()~ function.

  Since modules are not required to have a ~package.json~ file, not all modules
  are packages. Only modules that have a ~package.json~ file are also packages.

  In the  context of  a Node  program, the module  is also  the thing  that was
  loaded from a file.  in the following program:

  : var req = require('request')

  we might say that "The variable req refers to the request module".

*** Creating a package.json File
A ~package.json~ file:

 - lists the packages your project depends on
 - specifies versions  of a package  that your  project can use  using semantic
   versioning rules
 - makes your build reproducible.

**** Package Fields
A ~package.json~ file must contain "name" and "version" fields.

 - =name= ::

   The "name" field contains your package's name, and must be lowercase and one
   word, and may contain hyphens and underscores.

 - =version= ::

   The "version"  field must  be in  the form =x.x.x=  and follow  the semantic
   versioning guidelines.

 - =author= ::=

   If you want to include package author information in "author" field, use the
   following format (email and website are both optional):

   : Your Name <email@example.com> (http://example.com)

**** Creating a new package.json File
You can create a ~package.json~ file by running a CLI questionnaire or creating
a default ~package.json~ file.

- CLI Questionnaire ::

  To create  a ~package.json~ file  with values that  you supply, use  the ~npm
  init~ command.  Run the  following command  from the  root directory  of your
  package and answer the questions.

  : npm init

- Default ~package.json~ File ::

  To  create a  default  ~package.json~ using  information  extracted from  the
  current directory, use the ~npm init~ command with the =--yes= or =-y= flag.

  : npm init --yes

*** Creating a Node.js Module
~Node.js~ modules are a type of package that can be published to ~npm~.

1. Create a ~package.json~ file

2. Create the load file and create an ~exports~ function

*** Specifying Dependencies and DevDependencies
To  specify  the packages  your  project  depends on,  you  must  list them  as
"dependencies" or "devDependencies" in your package's ~package.json~ file. When
you (or another  user) run ~npm install~, ~npm~ will  download dependencies and
devDependencies  that  are listed  in  ~package.json~  that meet  the  semantic
version requirements listed for each.

- dependencies :: Packages required by your application in production.

  To add an entry to the  "dependencies" attribute of a ~package.json~ file, on
  the command line, run the following command:

  : npm install <package-name> [--save-prod]

- devDependencies ::  Packages that are  only needed for local  development and
  testing.

  To add an entry to the  "devDependencies" attribute of a ~package.json~ file,
  on the command line, run the following command:

  : npm install <package-name> --save-dev

*** npm CLI
- [[https://docs.npmjs.com/cli/v6/commands][npm CLI Commands]]

**** Latest Release
The latest release of ~npm~ is the most recent stable version. When you install
~Node.js~, ~npm~  is automatically installed.  However, ~npm~ is  released more
frequently than ~Node.js~, so to install the latest stable version of ~npm~, on
the command line, run:

: npm install npm@latest -g

** Webpack
- [[https://webpack.js.org][Webpack Home]]

*** Webpack Getting Started
- [[https://webpack.js.org/guides/getting-started/][Webpack Getting Started Page]]

~webpack~  is used  to  compile  JavaScript modules.  Once  installed, you  can
interact with webpack either from its CLI or API.

- [[https://webpack.js.org/api/cli/][CLI Site]]
- [[https://webpack.js.org/api/node/][API Site]]
- [[https://webpack.js.org/concepts/][Webpack Concepts]]

*** Webpack Concepts
- [[https://webpack.js.org/concepts/][Webpack Concepts Site]]

~webpack~ is a *static module bundler* for modern JavaScript applications. When
~webpack~ processes your application, it internally builds a /dependency graph/
which maps every module your project needs and generates one or more bundles.

**** Webpack Modules
~Node.js~ has supported modular programming  almost since its inception. On the
web, however, support for modules has been slow to arrive. Multiple tools exist
that support  modular JavaScript  on the  web, with a  variety of  benefits and
limitations. ~webpack~ builds on lessons learned from these systems and applies
the concept of modules to any file in your project.

In  contrast  to  ~Node.js~  modules,   ~webpack~  modules  can  express  their
dependencies in a variety of ways. ~webpack~ supports to following module types
natively:

- ES Modules :: An =ES2015= ~import~ statement
- CommonJS Modules :: A =CommonJS= ~require()~ statement
- AMD Modules :: An =AMD= ~define~ and ~require~ statement
- WebAssembly Modules :: An =@import= statement inside of a css/sass/less file.
- Assets ::   An image url in  a stylesheet ~url(...)~ or  HTML =<img src=...>=
  file.

***** Loaders
~webpack~ supports modules written in  a variety of languages and preprocessors
via  *loaders*.  *Loaders* describe  to  ~webpack~  how to  process  non-native
modules  and  include  these  dependencies into  your  bundles.  The  ~webpack~
community  has built  loaders  for  a wide  variety  of  popular languages  and
language processors, including:

- CoffeeScript
- TypeScript
- ESNext (Babel)
- Sass
- Less
- Stylus
- Elm

**** Entry

#+cindex:entry point
An *entry point* indicates which module  ~webpack~ should use to begin building
out  its internal  dependency  graph.  ~webpack~ will  figure  out which  other
modules and libraries that entry point depends on (directly and indirectly).

By default its  value is ~./src/index.js~, but you can  specify a different (or
multiple  entry  points)  by  setting  an =entry=  property  in  the  ~webpack~
configuration.

#+begin_src js
  // shorthand
  module.exports = {
    entry: './path/to/my/entry/file.js'
  };

  // object
  module.exports = {
    entry: {
      main: './path/to/my/entry/file.js'
    }
  };

  // scalable object
  module.exports = {
    entry: {
      app: './src/app.js',
      adminApp: './src/adminApp.js'
    }
  };

  // multi-main entry
  module.exports = {
    entry: [ 
      './src/file_1.js',
      './src/file_2.js'
    ],
    output: {
      filename: 'bundle.js'
    }
  };
#+end_src

**** Output
The =output= property tells ~webpack~ where  to emit the bundles it creates and
how to name these files.

It defaults  to ~./dist/main.js~ for the  main output file and  to the ~./dist~
folder for any other generated file.

You can configure this  part of the process by specifying  an =output= field in
your configuration. The  minimum requirement for the =output=  property in your
~webpack~  configuration is  to  set its  value  to an  object  and provide  an
=output.filename= to  use for  the =output=  file(s). This  configuration would
output a single ~bundle.js~ file into the ~dist~ directory.

#+begin_src js
  // minimum
  module.exports = {
    output: {
      filename: 'bundle.js',
    }
  };

  // minimum with path
  const path = require('path');

  module.exports = {
    entry: './path/to/my/entry/file.js',
    output: {
      path: path.resolve(__dirname, 'dist'),
      filename: 'my-first-webpack.bundle.js'
    }
  };
#+end_src

If your  configuration creates  more than  a single  "chunk" (as  with multiple
entry points  or when  using plugins like  CommonsChunkPlugin), you  should use
/substitutions/ to ensure that each file has a unique name.

#+begin_src js
  module.exports = {
    entry: {
      app: './src/app.js',
      search: './src/search.js'
    },
    output: {
      filename: '[name].js',
      path: __dirname + '/dist'
    }
  };

  // writes to disk: ./dist/app.js, ./dist/search.js
#+end_src

**** Loaders
*Loaders* allow webpack  to process other types of files  and convert them into
valid  modules that  can  be consumed  by  your application  and  added to  the
dependency graph.

At a high level, loaders have two properties in your webpack configuration:

1. =test= property:  identifies which file or files should be transformed.
2.  =use=  property:   indicates  which  loader  should  be  used   to  do  the
   transforming.

#+begin_src js
  const path = require('path');

  module.exports = {
    output: {
      filename: 'my-first-webpack.bundle.js'
    },
    module: {
      rules: [
	{ test: /\.txt$/, use: 'raw-loader' }
      ]
    }
  };
#+end_src

The configuration above has defined a =rules= property for a single module with
two required  properties: =test= and  =use=. This tells webpack's  compiler the
following:

- When ~webpack~ finds  a ~.txt~ file inside a ~require~  or ~import~ statement
  (a  module) use  the =raw-loader=  to transform  it before  adding it  to the
  bundle.
- It is  important to  remember that  when defining  =rules= in  your ~webpack~
  ~config~, you  are defining  them under =module.rules=  and not  =rules=. For
  your benefit, ~webpack~ will warn you if this is done incorrectly.

**** Plugins
Plugins  can  be leveraged  to  perform  a wider  range  of  tasks like  bundle
optimization, asset management and injection of environment variables.

In  order to  use a  plugin,  you need  to ~require()~  it  and add  it to  the
=plugins= array.

Most  plugins are  customizable through  options. Since  you can  use a  plugin
multiple times in a configuration for different purposes, you need to create an
instance of it by calling it with the ~new~ operator.

#+begin_src js
  const HtmlWebpackPlugin = require('html-webpack-plugin'); //installed via npm
  const webpack = require('webpack'); //to access built-in plugins

  module.exports = {
    module: {
      rules: [
	{ test: /\.txt$/, use: 'raw-loader' }
      ]
    },
    plugins: [
      new HtmlWebpackPlugin({template: './src/index.html'})
    ]
  };
#+end_src

In the example above, the ~html-webpack-plugin~ generates an HTML file for your
application by injecting automatically all your generated bundles.

**** Mode
By setting  the mode parameter to  either
- =development=,
- =production= or
- =none=,

you  can  enable  webpack's  built-in optimizations  that  correspond  to  each
environment. The default value is =production=.

#+begin_src js
  module.exports = {
    mode: 'production'
  };
#+end_src

** Babel
- [[https://babeljs.io][Babel Home]]

"Babel is a JavaScript compiler"

Babel is a toolchain that is mainly  used to convert ECMAScript 2015+ code into
a backwards compatible  version of JavaScript in current and  older browsers or
environments.

Babel:
- transforms syntax
- polyfills missing features in a target environment
- transforms source code

#+cindex:transformer
#+cindex:plugin
Babel  has  support  for  the  latest version  of  JavaScript  through  /syntax
transformers/. These /plugins/  allow you to use new syntax,  right now without
waiting for browser support.

#+cindex:preset
Babel can convert JSX syntax! Check out our /React preset/ to get started.

#+cindex:type annotations
Babel can  strip out /type  annotations/! Check out  either our Flow  preset or
TypeScript preset to get started.

#+cindex:plugin
Babel is built out of /plugins/. A  plugin is just a function. Compose your own
transformation pipeline using existing plugins or  write your own. Easily use a
set of plugins by using or creating a preset.

*** Overview
How to compile JavaScript application code that uses ES2015+ syntax into code
that works in current browsers and polyfill missing features.

1. Install Babel
   : npm install --save-dev @babel/core @babel/cli @babel/preset-env
   : npm install --save @babel/polyfill

2. Create a config  file ~babel.config.json~ in the root of  your project ( >
   =v7.8.0=)
   #+begin_src js
     {
       "presets": [
	 [
	   "@babel/env",
	   {
	     "targets": {
	       "edge": "17",
	       "firefox": "60",
	       "chrome": "67",
	       "safari": "11.1",
	     },
	     "useBuiltIns": "usage",
	     "corejs": "3.6.5",
	   }
	 ]
       ]
     }
   #+end_src

   Or ~babel.config.js~ if you are using an older Babel version

   #+begin_src js
     const presets = [
       [
	 "@babel/env",
	 {
	   targets: {
	     edge: "17",
	     firefox: "60",
	     chrome: "67",
	     safari: "11.1",
	   },
	   useBuiltIns: "usage",
	   "corejs": "3.6.4",
	 },
       ],
     ];

     module.exports = { presets };'
   #+end_src

3. Run this command to compile all your code from the ~src~ directory to ~lib~:

   : ./node_modules/.bin/babel src --out-dir lib
   : npx babel src --out-dir lib

*** Babel CLI
All the Babel modules you'll need are published as separate npm packages scoped
under ~@babel~ (since version 7). This  modular design allows for various tools
each designed  for a specific  use case. Here  we'll look at  ~@babel/core~ and
~@babel/cli~.

~@babel/cli~ is a tool  that allows you to use babel  from the terminal. Here's
the installation command and a basic usage example:

: npm install --save-dev @babel/core @babel/cli
: ./node_modules/.bin/babel src --out-dir lib

This will  parse all  the JavaScript  files in the  ~src~ directory,  apply any
transformations  we  have  told it  to,  and  output  each  file to  the  ~lib~
directory.  Since we  haven't told  it to  apply any  transformations yet,  the
output  code  will  be identical  to  the  input  (exact  code styling  is  not
preserved). We  can specify  what transformations  we want  by passing  them as
options.

We used  the =--out-dir=  option above. You  can view the  rest of  the options
accepted by the cli tool by running it with =--help=. But the most important to
us right now are =--plugins= and =--presets=.

*** Plugins and Presets
Transformations  come in  the form  of  *plugins*, which  are small  JavaScript
programs that instruct Babel on how to carry out transformations to the code.

To transform =ES2015+=  syntax into =ES5= we can rely  on official plugins like
~@babel/plugin-transform-arrow-functions~:

: npm install --save-dev @babel/plugin-transform-arrow-functions
: ./node_modules/.bin/babel src --out-dir lib \
:      --plugins=@babel/plugin-transform-arrow-functions

Now any  arrow functions in  our code will  be transformed into  ES5 compatible
function expressions.

But we also have other =ES2015+= features in our code that we want transformed.
Instead of  adding all the plugins  we want one by  one, we can use  a "preset"
which is just a pre-determined set of plugins.

For our use case here, there's an excellent preset named =env=.

: npm install --save-dev @babel/preset-env
: ./node_modules/.bin/babel src --out-dir lib \
:      --presets=@babel/env

Without  any configuration,  this preset  will include  all plugins  to support
modern JavaScript (=ES2015=, =ES2016=, etc.). But presets can take options too.
Rather than  passing both =cli= and  =preset= options from the  terminal, let's
look at another way of passing options: configuration files.

#+cindex:configuration, babel
#+pindex:babel.config.json
Create a file  called ~babel.config.json~ (requires v7.8.0 and  above) with the
following content:

#+begin_src js
  {
  "presets": [
    [
    "@babel/env",
      {
	"targets": {
	  "edge": "17",
	  "firefox": "60",
	  "chrome": "67",
	  "safari": "11.1"
	  }
	}
      ]
    ]
  }
#+end_src

Now the  =env= preset will only  load transformation plugins for  features that
are not available in our target browsers.  We're all set for syntax.

*** Polyfills
Directly  including  core-js/stable  (to   polyfill  ECMAScript  features)  and
regenerator-runtime/runtime (needed to use transpiled generator functions):

: import "core-js/stable";
: import "regenerator-runtime/runtime";

This  means you  can  use new  built-ins like  =Promise=  or =WeakMap=,  static
methods   like  ~Array.from~   or   ~Object.assign~,   instance  methods   like
~Array.prototype.includes~,  and generator  functions  (when  used alongside  the
regenerator plugin).  The polyfill adds to  the global scope as  well as native
prototypes like =String= in order to do this.

We're using the =env= preset which has  a "useBuiltIns" option that when set to
"usage" will practically apply the  last optimization mentioned above where you
only include  the polyfills you  need. With  this new option  the configuration
changes like this:

#+begin_src js
  {
    "presets": [
      [
	"@babel/env",
	{
	  "targets": {
	    "edge": "17",
	    "firefox": "60",
	    "chrome": "67",
	    "safari": "11.1",
	  },
	  "useBuiltIns": "usage",
	}
      ]
    ]
  }
#+end_src

Babel will  now inspect  all your code  for features that  are missing  in your
target environments and include only the required polyfills.

*** Configuration
**** babel.config.json
Create a file called ~babel.config.json~ with the following content at the root
of your project (where the ~package.json~ is).

#+begin_src js
  // json
  {
    "presets": [...],
    "plugins": [...]
  }

  // js

  module.exports = function (api) {
    api.cache(true);

    const presets = [ ... ];
    const plugins = [ ... ];

    return {
      presets,
      plugins
    };
  }
#+end_src
**** .babelrc.json

Create  a  file called  ~.babelrc.json~  with  the  following content  in  your
project.

#+begin_src js
  {
    "presets": [...],
    "plugins": [...]
  }
#+end_src

**** package.json
Alternatively,  you can  choose  to specify  your  ~.babelrc.json~ config  from
within ~package.json~ using the =babel= key like so:

#+begin_src js
  {
    "name": "my-package",
    "version": "1.0.0",
    "babel": {
      "presets": [ ... ],
      "plugins": [ ... ],
    }
  }
#+end_src

**** JavaScript configuration files
You  can  also  write   ~babel.config.json~  and  ~.babelrc.json~  files  using
JavaScript:

#+begin_src js
  const presets = [ ... ];
  const plugins = [ ... ];

  module.exports = { presets, plugins };
#+end_src

*** Using the CLI
: babel --plugins @babel/plugin-transform-arrow-functions script.js

*** Using the API---@babel/core
#+begin_src js
  require("@babel/core").transform("code", {
    plugins: ["@babel/plugin-transform-arrow-functions"]
  });
#+end_src

** Testing
* Build Tools
:PROPERTIES:
:appendix: t
:custom_id: build-tools
:END:
** Makefile					:dependencies:env_vars:perl:
:PROPERTIES:
:appendix: t
:dependency1: make
:dependency2.0: AWS User account at https://aws.amazon.com
:dependency2.1: AWS cli v2 in PATH https://docs.aws.amazon.com/cli/index.html
:dependency2.2: See how to Install AWS CLI v2 at https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html
:dependency2.3: aws credentials: access token and secret access token stored in ~/.aws/credentials
:dependency2.4: AWS S3 buckets set up for serving a static web page
:dependency3: GitHub Account with personal access token stored in GITHUB_TOKEN
:dependency4: texinfo @6.7._
:dependency5: Emacs, Org-mode, Babel language 'shell' enabled
:env_var1: SYNC_ORG_TEMPLATE: holds the full path to this Template.org file
:env_var2: GITHUB_TOKEN: holds the GitHub personal access token
:env_var3: EDITOR: must hold a reference to a working emacsclient server
:env_var4: COLORS
:END:

#+pindex:Makefile
#+name:Makefile
#+header: :tangle Makefile
#+begin_src makefile

  ###############################################################################
  ### USER-DEPENDENT VARIABLES
  ### USE ENVIRONMENT VARIABLES WHENEVER POSSIBLE

  # NOTE: All environment variables need to be exported PRIOR to starting the
  # Emacs server as EDITOR in your shell startup files; otherwise, they will not
  # be available to Emacs.
  # When I moved from using Bash to Zsh, I inadvertently changed the order of
  # import, and started the Emacs server before importing, and caused a horrible
  # bug which caused the program to work on one computer but fail on another.

  # The absolute path to this Template file
  TEMPLATE := $(SYNC_ORG_TEMPLATE)


  ### TOOLS & RESOURCES
  # tools is a directory holding tangled scripts, such as cmprpl
  # resources is a directory holding static resources for the project
  # images is a directory holding jpg and png image files
  RESOURCES := resources
  TOOLS	    := $(RESOURCES)/tools
  IMAGES    := $(RESOURCES)/images
  CMPRPL    := $(TOOLS)/cmprpl

  # Use emacsclient as $EDITOR; make sure it is set in a shell startup file and
  # the server has been started.
  EMACS	  := $(EMACS)
  EDITOR  := $(EDITOR)

  # User’s personal GitHub token for authentication to GitHub
  # DO NOT HARD-CODE THIS VALUE
  GITHUB_TOKEN := $(GITHUB_TOKEN)

  # The AWS Command Line Interface (AWS CLI) is an open source tool
  # that enables you to interact with AWS services using commands in
  # your command-line shell.  It must be present on your system.  Run the 'make'
  # command 'install-aws-cli' to install it if you do not have it.  Be sure to
  # run 'aws configure' after installing it.  This will place your AWS
  # credentials into ~/.aws/credentials.
  AWS := aws
  S3  := $(AWS) s3
  CFD := $(AWS) cloudfront

  ### END OF USER-DEPENDENT VARIABLES
  ###############################################################################
  ### MAKE-GENERATED VARIABLES

  ### PROJ AND ORG
  # ORG is the name of this Org file with extension .org
  # PROJ is the project name---the Org file name without extension.

  ### NOTE: there can be only one Org file in the project directory;
  # so far this has not been a problem, but it might be.

  PWD  := $(shell pwd)
  ORG  := $(shell ls *.org)
  PROJ := $(basename $(ORG))

  ### NOTE: S is needed only for the Template file because of the way it is nested
  # one level deep in the Templates GitHub repo, which uses the plural form
  # of Templates, whereas this file uses the singular form, Template.  So when
  # the homepage link is updated, the curl command must be told to use the plural
  # form.	 This is obviously a hack only for my own use and can be removed once
  # I clean up this anomaly.

  ifeq ($(PROJ),$(basename $(notdir $(TEMPLATE))))
  S := s
  endif

  # The AWS S3 bucket to use to store the html source file; it is found at the
  # key #+bucket towards the beginning of the file and should include the appropriate
  # suffix (.com, .net, .org, etc)
  BUCKET       := $(shell $(EDITOR) --eval \
		 '(with-current-buffer (find-file-noselect "$(ORG)") \
		    (save-excursion \
		      (goto-char (point-min)) \
		      (re-search-forward "^\#[+]bucket:\\(.*\\)$$" nil t) \
		      (match-string-no-properties 1)))')
  S3_BUCKET    := s3://$(BUCKET)

  # Buckets set up to serve static web sites from S3 can use either http
  # or https protocols; some  http protocols will automatically redirect
  # to https;  however, some only use  http. I would like  to accomodate
  # both, and  so this code  finds the url's  that are in  my Cloudfront
  # account, which presumably will serve https.  If the url is not here,
  # then this must be set up to serve http instead.
  HTTP_S := $(shell $(CFD) list-distributions | perl -MJSON::PP -e \
	  '$$/=""; \
	   my @urls = (); \
	   my $$json=JSON::PP->new->decode(<STDIN>); \
	   for my $$item ( @{$$json->{"DistributionList"}{"Items"}} ) { \
		  push @urls, @{$$item->{"Aliases"}{"Items"}}; \
	   } \
	  my $$found = grep { /'$(BUCKET)'/ } @urls; \
	  print "http", ($$found ? "s" : "");')

  HTTPS_BUCKET := https://$(BUCKET)

  ### DIR, SRC
  # DIR is the .info name found at '#+texinfo_filename:<DIR>.info' (at
  # the bottom of this file in the export configuration settings)
  # without its extension, used as the INFO filename and the name of the
  # HTML export directory; this code uses the lowercased PROJ name if
  # there is no '#+texinfo_filename'.
  # SRC is HTML directory based upon the DIR name

  #DIR := $(shell $(EDITOR) --eval \
  #	'(with-current-buffer (find-file-noselect "$(ORG)") \
  #		(save-excursion \
  #		(goto-char (point-min)) \
  #		(re-search-forward "^\#[+]\\(?:texinfo_filename\\|TEXINFO_FILENAME\\):\\(.*\\).info$$" nil t) \
  #		(match-string-no-properties 1)))')

  DIR := $(shell sed -E -n "/^\#\+texinfo_filename/s/^.*:(.*)\.info$$/\1/p" $(ORG))
  ifeq ($(DIR),$(EMPTY))
	  DIR := $(shell echo $(PROJ) | tr "[:upper:]" "[:lower:]")
  endif

  SRC := $(DIR)/

  ### VERS: v1.2.34/
  # VERS is the version number of this Org document.
  # When sync is run after the version number has been updated, then VERS
  # picks up the newly-changed value.  VERS used to be staticly imbedded
  # when the Makefile was tangled, but it needs to be dynamic for
  # development.

  # QUERY: should this number be formatted like this, or should it be just the numbers?
  # The reason it includes them is the S3PROJ obtains the name from the S3 bucket, and
  # it includes them.  But it only includes them because I have made it so.  Not a good
  # reason just by itself.  The ending slash is not actually a part of the version, but
  # comes from the way the 'aws2 ls' command returns its values.	So VERS should probably
  # not include the trailing slash, although it doesn’t hurt anything.

  VERS := v$(shell $(EDITOR) --eval \
	  '(with-current-buffer (find-file-noselect "$(ORG)") \
		  (save-excursion \
		    (goto-char (point-min)) \
		    (re-search-forward "^\#[+]\\(?:macro\\|MACRO\\):version Version \\(\\(?:[[:digit:]]+[.]?\\)\\{3\\}\\)") \
		    (match-string-no-properties 1)))')/

  ### AWS
  # PROJ_LIST contains the list of projects currently uploaded to
  # the S3 bucket; each item contains the name of the project and its
  # current version.

  # Created function using elisp instead of the shell.
  # This variable contains an elisp list of strings of the form '("proj1-v1.2.3/" "proj2-v4.5.6/" ...)'
  # However, when it prints to the shell, the quotes are lost.
  # Need to make sure elisp's variable 'exec-path contains the proper $PATH instead of adding to 'exec-path.

  PROJ_LIST := $(shell $(EDITOR) --eval \
	  "(progn \
		  (require (quote seq)) (add-to-list (quote exec-path) (quote \"/usr/local/bin\")) \
		  (seq-map (lambda (s) (replace-regexp-in-string \"^\s+PRE \" \"\" s)) \
			  (seq-filter (lambda (s) (string-match-p (regexp-quote \" PRE \") s)) \
			  (process-lines \"$(AWS)\" \"s3\" \"ls\" \"$(S3_BUCKET)\"))))")

  ### S3PROJ
  # The name of the current project as obtained from S3: 'proj-v1.2.34/'
  # If there is no current project in the S3 bucket, then assign a value equal to
  # the Org project and version instead.  It is set to the project if found, and
  # NO if not found, then updated in the ifeq block below.
  S3PROJ := $(shell $(EDITOR) --eval \
		  '(let ((proj (seq-find (lambda (s) (string-match-p "$(DIR)" s)) (quote $(PROJ_LIST))))) \
		     (or proj (quote NO)))')

  ### PROJINS3
  # is used by make sync; this allows the index.html file to be generated the first
  # time the project is synced.  It is set to NO if this project is not currently in an
  # S3 bucket, and it is set to YES if it is.
  PROJINS3 :=

  ### S3VERS
  # The version of this project currently installed in the S3 bucket: 'v1.2.34/'
  # If there is no current version in the S3 bucket, then assign the version from
  # this Org file instead.
  S3VERS   :=

  # Update S3PROJ, S3VERS, and PROJINS3
  ifeq ($(S3PROJ), NO)
	  S3PROJ := $(DIR)-$(VERS)
	  S3VERS := $(VERS)
	  PROJINS3 := NO
  else
	  S3VERS := $(subst $(DIR)-,,$(S3PROJ))
	  PROJINS3 := YES
  endif

  ### GITHUB
  # USER is the current user's GitHub login name.

  # The user name used to be statically embedded into the Makefile
  # during tangle, but in an effort to make the Makefile dynamically
  # indepedent, dynamic code has replaced the static code.  The code
  # that placed the static name in the Makefile was a 'node' script that
  # ran in a separate Org process during tangle.	An unfortunate fact of
  # 'make' is that 'make' strips the quote marks from the string
  # obtained from the 'curl' command when the 'make shell' command
  # returns the string.	 This makes the string malformed JSON and
  # unparsable by most JSON parsers, including 'node’.	However,
  # 'perl'’s core module JSON::PP (but not JSON::XS) has facilities to
  # parse very malformed JSON strings.	Therefore, this dynamic code
  # uses 'perl' and the core module JSON::PP to parse the 'curl' string
  # into a 'perl' JSON object which can return the login name.	This
  # code should work with any version of 'perl' without having to
  # install any modules.

  USER	:= $(shell \
	    curl -sH "Authorization: token $(GITHUB_TOKEN)" https://api.github.com/user \
	    | \
	    perl -MJSON::PP -e \
		'$$/ = ""; \
		 my $$json = JSON::PP->new->loose->allow_barekey->decode(<STDIN>); \
		 print $$json->{login};' \
	    )
  SAVE		:= resources

  ### TEXINFO
  TEXI		:= $(PROJ).texi
  INFO		:= $(DIR).info
  INFOTN	:= $(shell $(EDITOR) --eval "(file-truename \"$(INFO)\")")
  PDF		:= $(PROJ).pdf
  INDEX		:= index.html
  HTML		:= $(DIR)/$(INDEX)
  DIR_OLD	:= $(DIR)-old

  ### AWS S3
  DST_OLD	:= $(S3_BUCKET)/$(S3PROJ)
  DST_NEW	:= $(S3_BUCKET)/$(DIR)-$(VERS)
  EXCL_INCL	:= --exclude "*" --include "*.html"
  INCL_IMAGES	:= --exclude "*" --include "*.jpg" --include "*.png"
  GRANTS	:= --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers
  S3SYNC	:= $(S3) sync --delete $(EXCL_INCL) $(SRC) $(DST_OLD) $(GRANTS)
  S3MOVE	:= $(S3) mv --recursive $(DST_OLD) $(DST_NEW) $(GRANTS)
  S3COPY	:= $(S3) cp $(INDEX) $(S3_BUCKET) $(GRANTS)
  S3REMOVE	:= $(S3) rm $(S3_BUCKET)/$(S3PROJ) --recursive
  S3IMAGESYNC	:= $(S3) sync $(INCL_IMAGES) $(IMAGES) $(S3_BUCKET)/$(IMAGES) $(GRANTS)

  ###############################################################################

  default: check texi info html pdf

  PHONY: default all check values boot \
	    texi info html pdf \
	    open-org open-texi open-html open-pdf \
	    clean dist-clean wiped-clean \
	    help sync update delete-proj \
	    install-aws-cli \
	    index-html upload-index-html

  values: check
	    @printf "$${BLUE}Values...$${CLEAR}\n"
	    @echo TEMPLATE:	$(TEMPLATE)
	    @echo EDITOR:	$(EDITOR)
	    @echo USER:		$(USER)
	    @echo PWD:		$(PWD)
	    @echo ORG:		$(ORG)
	    @echo TEXI:		$(TEXI)
	    @echo INFO:		$(INFO)
	    @ECHO INFOTN:	$(INFOTN)
	    @echo BUCKET:	$(BUCKET)
	    @echo PROJ:		$(PROJ) $S
	    @echo S3_BUCKET:	$(S3_BUCKET)
	    @echo HTTP_S:	$(HTTP_S)
	    @echo HTTPS_BUCKET:	$(HTTPS_BUCKET)
	    @echo VERS:		$(VERS)
	    @echo S3PROJ:	$(S3PROJ)
	    @echo S3VERS:	$(S3VERS)
	    @echo DIR:		$(DIR)
	    @echo DIR_OLD:	$(DIR_OLD)
	    @echo SRC:		$(SRC)
	    @echo DST_OLD:	$(DST_OLD)
	    @echo DST_NEW:	$(DST_NEW)
	    @echo PROJ_LIST:	"$(PROJ_LIST)"
	    @echo PROJINS3:	$(PROJINS3)

  check:
	    @printf "$${BLUE}Checking dependencies...$${CLEAR}\n"

	    @[[ -z $(BUCKET) ]] && \
	       { printf "$${RED}$(BUCKET) $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}BUCKET: $${GREEN}$(BUCKET)$${CLEAR}\n";

	    @[[ -z $${GITHUB_TOKEN} ]] && \
	       { printf "$${RED}GITHUB_TOKEN $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}GITHUB_TOKEN: $${GREEN}SET$${CLEAR}\n";

	    @[[ (-d ~/.aws) && (-f ~/.aws/credentials) && (-f ~/.aws/config) ]] && \
	       printf "$${CYAN}AWS credentials and config: $${GREEN}SET$${CLEAR}\n" || \
	       { printf "$${RED}~/.aws 'credentials' and 'config' must be set.$${CLEAR}\n"; exit 1; }

	    @[[ "$(shell $(EDITOR) --eval '(member (quote texinfo) org-export-backends)')" = "(texinfo)" ]] && \
		  printf "$${CYAN}Texinfo backend: $${GREEN}INSTALLED.$${CLEAR}\n" || \
		  { printf "$${YELLOW}Texinfo backend:$${CLEAR} $${RED}NOT INSTALLED; it must be installed.$${CLEAR}\n"; exit 1; }

	    @[[ $(shell $(EDITOR) --eval '(symbol-value org-confirm-babel-evaluate)') == "t" ]] && \
		  { printf "$${YELLOW}org-confirm-babel-evaluate:$${CLEAR} $${RED}T; set to NIL.$${CLEAR}\n"; exit 1; } || \
		  printf "$${CYAN}org-confirm-babel-evaluate: $${GREEN}OFF.$${CLEAR}\n\n"

  open-org: $(ORG)
	    @$(EDITOR) -n $(ORG)
  $(ORG):
	    @echo 'THERE IS NO $(ORG) FILE!!!'
	    exit 1

  texi: $(TEXI)
  $(TEXI): $(ORG)
	   @echo Making TEXI...
	   @$(EDITOR) -u --eval \
		  "(with-current-buffer (find-file-noselect \"$(ORG)\" t) \
			  (save-excursion \
			  (org-texinfo-export-to-texinfo)))"
	   @echo Done making TEXI.
  open-texi: texi
	   @$(EDITOR) -n $(TEXI)

  info: $(INFO)
  $(INFO): $(TEXI)
	   @echo Making INFO...
	   @makeinfo -o $(INFO) $(TEXI)
	   @$(EDITOR) -u -eval \
		  "(when (get-buffer \"$(INFO)\") \
			  (with-current-buffer (get-buffer \"$(INFO)\") \
				  (revert-buffer t t t)))"
	   @echo Done making INFO.

  open-info: info
	   @$(EDITOR) -u -eval \
		  "(if (get-buffer \"*info*\") \
			  (with-current-buffer (get-buffer \"*info*\") \
				(when (not (string= \"(symbol-value (quote Info-current-file))\" \"$(INFOTN)\")) \
					(info \"$(INFOTN)\")) \
				(revert-buffer t t t)) \
		      (info \"$(INFOTN)\"))"

  html: $(HTML)
  $(HTML): $(TEXI)
	   @echo Making HTML INFO..
	   @makeinfo --html -o $(DIR) $(TEXI)
	   @echo Done making HTML.
	   $(CMPRPL) $(DIR) $(DIR_OLD)
  open-html: html
	   @open $(HTML)

  # If pdftexi2dvi produces an error, it may still produce a viable PDF;
  # therefore, use --tidy.  If it produces an error, try to link the PDF;
  # if it does not produce an error, the PDF will be added to the top dir
  # and there will be no attempt to link.
  pdf:	$(PDF)
  $(PDF): $(TEXI)
	  @echo Making PDF INFO...
	  @-pdftexi2dvi --quiet --build=tidy $(TEXI) || ln -s $(PROJ).t2d/pdf/build/$(PDF) $(PDF)
	  @echo Done making PDF.
  open-pdf:pdf
	   @open $(PDF)

  sync:   $(HTML)
	  @echo Syncing version $(VERS) onto $(S3VERS)...
	  $(S3SYNC)
	  $(S3IMAGESYNC)
	  @echo Done syncing.
	  [[ $(VERS) != $(S3VERS) ]] && { echo Moving...; $(S3MOVE); echo Done moving.;  make homepage; } || :
	  [[ $(PROJINS3) = "NO" ]] && make homepage || :

  # This is a target-specific variable for updating the “description”
  # key on the GitHub repo page with the current version number.  It
  # first makes a curl call to the GitHub project repo, finds the
  # “description” line, pulls out the description only (leaving the old
  # version) and then prints the value with the current version number.
  # This value is used by the “homepage:” target in the PATCH call.
  # This method is arguably harder to code but faster to run than using
  # Perl with the JSON::PP module.

  homepage: description = $(shell \
	  curl -s \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S | \
		  (perl -ne 'if (/^\s*\"description\":\s*\"(.*): v(?:(?:[[:digit:]]+[.]?){3})/) {print $$1}'))

  ### NOTE the use of the S variable at the end of PROJ; this is to handle
  # the singular case of the GitHub repo using the plural form, Templates
  # whereas the the Template.org file uses the singular form.
  homepage: $(ORG) upload-index-html
	    @echo Updating homepage...
	    @echo DESCRIPTION: $(description)
	    @echo VERS: $(VERS)
	    @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Content-Type: application/json" \
		  -X PATCH \
		  -d "{\"homepage\":\"$(HTTPS_BUCKET)/$(DIR)-$(VERS)\",\
		       \"description\":\"$(description): $(VERS)\"}" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	    @echo Done updating homepage.

  delete-proj:
	  @echo Deleting project $(PROJ)...
	  @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Accept: application/vnd.github.v3+json" \
		  -X DELETE \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	  @$(S3REMOVE)
	  @make dist-clean
	  @make upload-index-html
	  @$(EDITOR) -u --eval "(kill-buffer \"$(ORG)\")"
	  @rm -rf "../$(PROJ)"
	  @echo Done deleting project.

  index-html: $(INDEX)
  $(INDEX): $(ORG)
	  @echo making index.html...
	  $(EDITOR) --eval \
	  "(with-current-buffer (find-file-noselect \"$(ORG)\") \
		  (save-excursion \
		    (org-link-search \"#project-index-title\") \
		    (org-export-to-file (quote html) \"index.html\" nil t)))"
	  @echo Done making index.html.

  upload-index-html: $(INDEX)
	   @echo Uploading index.html...
	   $(S3COPY)
	   @echo Done uploading index.html

  install-aws-cli:
	    curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg" && \
	    sudo installer -pkg AWSCLIV2.pkg -target / && \
	    which aws && aws --version
	    rm -rf AWSCLIV2.pkg

  clean:
	  @echo Cleaning...
	    -@rm *~ 2>/dev/null
	    -@for file in *.??*; \
	    do \
		    ext=$${file#$(PROJ).}; \
		    [[ ! $${ext} =~ org|texi|info|pdf|html ]] && rm -rv $${file}; \
	    done

  dist-clean: clean
	  @echo Dist Cleaning...
	    @${EDITOR} -u --eval \
	      "(kill-buffer \"$(ORG)\")"
	    -@rm -rf *.{texi*,info*,html*,pdf*} $(DIR) $(TOOLS)
	    -@for dir in *; \
		do \
		    [ -d $$dir -a $$dir != "$(DIR_OLD)" -a $$dir != $(SAVE) ] && \
		    rm -vr $$dir; \
		done

  wipe-clean: dist-clean
	  @echo Wipe Clean...
	    -@rm -rf Makefile Readme.md $(DIR_OLD)
	    @git checkout Makefile README.md

  git-ready: dist-clean
	    git checkout Makefile
	    git checkout README.md
	    git status

  help:
	    @echo '"make boot" tangles all of the files in Template'
	    @echo '"make default" makes the .texi file, the .info file, \
	    the html files, and the .pdf file.'
	    @echo

	    @echo '"make check" checks for prerequistes'
	    @echo '"make values" runs check and prints variable values'
	    @echo

	    @echo '"make texi" makes the .texi file'
	    @echo '"make info" makes the .info file'
	    @echo '"make html" makes the html distribution in a subdirectory'
	    @echo '"make pdf" makes the .pdf file'
	    @echo

	    @echo '"make open-org" opens the ORG program using emacsclient for editing'
	    @echo '"make open-texi" opens the .texi file using emacsclient for review'
	    @echo '"make open-html" opens the distribution index.html file \
	    in the default web browser'
	    @echo '"make open-pdf" opens the .pdf file'
	    @echo

	    @echo '"make sync" syncs the html files in the AWS S3 bucket BUCKET; \
	    you must have your AWS S3 bucket name in the env var AWS_S3_BUCKET; \
	    You must have your AWS credentials installed in ~/.aws/credentials'
	    @echo

	    @echo '"make install-aws-cli" installs the "aws cli v2" command-line tools'
	    @echo 'You also need to run "aws configure" and supply your Access Key and Secret Access Key'
	    @echo

	    @echo '"make clean" removes the .texi, .info, and backup files ("*~")'
	    @echo '"make dist-clean" cleans, removes the html distribution, \
	    and removes the build directory'
	    @echo '"make wipe-clean" wipes clean the directory, including old directories'
	    @echo

	    @echo '"make delete-proj" deletes the project from the file system, GitHub and AWS'

#+end_src

*** TODO Next
1. The CloudFront configuration needs to be updated recognize the new version
   directory that is created as part of the ~sync~ operation.

2. Update the GitHub HOME website link for each new sync operation.

3. Store on GitHub a version of each other format upon a sync operation (i.e.,
   the INFO and PDF versions)

** Compare Replace

#+begin_comment
The following source code tangles all files during an export operation. This is
to  make  sure  the  ~cmprpl~  source code  exists  in  the  ~resources/tools/~
directory before running  the Makefile target =html=. It also  makes sure there
is a Makefile on an initial export. The following code is not exported.
#+end_comment

#+name:tangle-org-file
#+header: :exports results :eval yes :results silent
#+begin_src emacs-lisp
(org-babel-tangle-file (buffer-file-name))
#+end_src

The  AWS ~sync~  command  relies  upon time  stamps  to  determine whether  two
programs are identical or not, as  well as content.  If two otherwise identical
files have  different time stamps,  ~sync~ will  assume they are  different and
will  process the  newer.   However, the  ~texinfo~  ~makeinfo --html~  command
produces all  new files even  if some files  (or most files)  remain unchanged.
This  means that  all files  will be  uploaded to  the AWS  S3 bucket  on every
iteration, even though the majority of the files are actually unchanged.

The ~cmprpl~  source code attempts to  resolve the issue of  identical exported
code having different  time stamps, thus defeating the benefit  provided by the
~aws2 s3 sync~ command uploading only changed files.

This program makes sure that a generated HTML directory exists: =$DIR_NEW=.  If
it doesn’t, then it is in an improper state and the program stops with an error
message.

The  program then  checks  if  an old  directory  exists,  =$DIR_OLD=.  If  one
doesn’t,  then one  is  created by  copying the  current  new directory.   This
provides a baseline  for comparisons going forward.  The program  exits at that
point. It is very important that  the =$DIR_OLD= directory not be deleted going
forward.

Given  that =$DIR_OLD=  exists, the  program then  loops through  all files  in
=$DIR_NEW= and  compares them  to the  files in =$DIR_OLD=.   If the  files are
identical, the =$DIR_OLD= file replaces the =$DIR_NEW= file while retaining the
old time stamp (using the ~-p~ option of ~cp~. If a file is different, then the
=$DIR_NEW= file  replaces the =$DIR_OLD=  file, thus giving it  updated content
and  an updated  time stamp.   If the  file does  not exist  in the  =$DIR_OLD=
directory, then it is added.

The  program then  loops through  all of  the files  in the  old directory  and
deletes  any that  do not  exist in  the new  directory.  Now  both directories
should be in sync.

#+caption:Compare Replace program
#+name:cmprpl
#+header: :mkdirp t
#+header: :shebang "#!/usr/bin/env bash"
#+begin_src sh :tangle resources/tools/cmprpl
  [[ $# -eq 2 ]] || { echo "ERROR: Incorrect command line arguments"; exit 1; }
  DIR_NEW=$1
  DIR_OLD=$2

  [[ -d $DIR_NEW ]] || { echo "ERROR: $DIR_NEW does not exist"; exit 1; }
  [[ -d $DIR_OLD ]] || { echo "CREATING: $DIR_OLD does not exist"; cp -a $DIR_NEW $DIR_OLD; exit 0; }

  for newfile in $DIR_NEW/*
  do
      oldfile=$DIR_OLD/$(basename $newfile)
      if [[ -e $oldfile ]]
      then
	 if cmp -s $newfile $oldfile
	 then
	     printf "${GREEN}copying OLD to NEW${CLEAR}: "
	     cp -vp $oldfile $newfile
	 else
	     printf "${PURPLE}copying NEW to OLD${CLEAR}: "
	     cp -vp $newfile $oldfile
	 fi
      else
	  printf "${BLUE}creating NEW in OLD${CLEAR}: "
	  cp -vp $newfile $oldfile
      fi
  done

  for oldfile in $DIR_OLD/*
  do
      newfile=$DIR_NEW/$(basename $oldfile)
      if [[ ! -e $newfile ]]
      then
	  printf "${RED}removing OLD${CLEAR}: "
	  rm -v $oldfile
      fi
  done
#+end_src


** Update Utility Commands
*** Get Parsed Org Tree
This function looks for an Org file in the present working directory, and if it
finds one returns  a parsed tree using  ~org-element-parse-buffer~.  It returns
=nil= if there is no Org file or if the found file is not in ~org-mode~.

#+name:get-parsed-org-tree
#+header: :results silent
#+begin_src emacs-lisp
(defun get-parsed-org-tree (&optional org-dir)
  "This function takes an optional directory name, changes to
that directory if given, otherwise uses the pwd, and finds an Org
file and returns its parsed tree, or nil if none found."
  (when org-dir
      (cd (file-name-as-directory org-dir)))
  (let ((buf (car-safe (find-file-noselect "*.org" nil nil t))))
    (if buf
	(with-current-buffer buf (org-element-parse-buffer))
      nil)))
#+end_src

*** Check for CID
This code  checks whether an  Org file contains  a =custom_id= of  a particular
value.  It accepts  a ~cid-value~ and an optional directory.   If the directory
is not given, then it defaults to the current directory.  If throws an error if
the directory does not exist.  It returns =nil= if the given directory does not
contain an Org file.   It returns =t= if the Org file  contains a node property
of   =custom_id=  and   value  ~cid-value~,   or   =nil=  if   not.   It   uses
~get-parsed-org-tree~.

#+name:org-tree-cid-p
#+header: :results silent
#+begin_src emacs-lisp
(defun org-tree-cid-p (cid-value &optional org-dir)
  "Check whether an org file contains a custom_id of CID"
  (let ((tree (get-parsed-org-tree org-dir)))
    (car (org-element-map tree 'property-drawer
	   (lambda (pd) (org-element-map (org-element-contents pd) 'node-property
			  (lambda (np)
			    (and
			     (string= "custom_id" (org-element-property :key np))
			     (string= cid-value (org-element-property :value np))))))
	   nil t))))
#+end_src

#+name:run-org-tree-cid-p
#+header: :var cid="build-tools"
#+header: :var dir="/usr/local/dev/programming/MasteringEmacs"
#+header: :var gpot=get-parsed-org-tree()
#+header: :var otcp=org-tree-cid-p()
#+header: :results value
#+header: :eval never-export
#+begin_src emacs-lisp
(org-tree-cid-p cid dir)
#+end_src

#+call: run-org-tree-cid-p(dir="/usr/local/dev/programming/MasteringEmacs")

** Bucket Index HTML
The bucket should contain a master ~index.html~  file that links to each of the
individual project  ~index.html~ files.  The  master ~index.html~ file  will be
placed at the root of  the bucket, ~https://<bucket-name>.com/~, and the bucket
must be set up to serve this ~index.html~ when the user hits the root.

*** Get Bucket Name
 This  code searches  for  the keyword-value  pair =bucket:<BUCKET-NAME>=  that
 should be  located towards the  beginning of the  file, and returns  the value
 =BUCKET-NAME= or nil if not found.

#+name: get-bucket-name
#+header: :results value
#+begin_src emacs-lisp
   (save-excursion
     (goto-char (point-min))
     (re-search-forward "^#\\+bucket:\\s*?\\(.*\\)$" nil t)
     (match-string-no-properties 1))
#+end_src

For some reason, ~get-bucket-name~ does not  work when called from the headline
[[#project-index-links][=Links for  bucket=]] below  when creating  =index.html=, even  if it  returns as
~(prin1 ...)~ and is  set up to ~:return output~; the  call receives =nil=. The
following code from ~bucket-name~, however, works. I don't know why.

#+name: bucket-name
#+header: :results output
#+header: :var bucket-name=get-bucket-name()
#+begin_src emacs-lisp
(prin1 bucket-name)
#+end_src

*** Bucket HTTPS URL
This  code calls  ~get-bucket-name~ and  returns the  value returned  as a  URL
string or nil.

#+name: bucket-https-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "https://" b)
#+end_src

*** S3 Bucket URL
This code calls ~get-bucket-name~ and returns the AWS S3 bucket url.

#+name: s3-bucket-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "s3://" b)
#+end_src

*** Bucket Projects List
This code uses the ~s3-bucket-url~ result to obtain the list of projects in the
bucket.  It does  this by calling the  AWS S3 high-level command  ~ls~ and then
removing the  =PRE= string in  each result.  The result  that is returned  is a
single  string that  can be  separated into  individual links  by breaking  the
string on spaces.

#+name: bucket-projects-list
#+header: :results output
#+header: :var bucket=s3-bucket-url()
#+begin_src sh
/usr/local/bin/aws s3 ls ${bucket} | sed -ne 's/^.*PRE //p'
#+end_src

*** Bucket Project Links
This code  uses the result  from ~bucket-projects-list~ to create  an unordered
list of  links written to  bucket projects, written  in Org-mode syntax.  It is
executed by a =#+call:= in [[*Bucket Index][*Bucket  Index]] during an HTML export of that subtree
to a file called =index.html=.

#+name: bucket-project-links
#+header: :var b-url=bucket-https-url()
#+header: :var projects=bucket-projects-list()
#+header: :results output raw
#+begin_src emacs-lisp
(seq-do (lambda (u) (princ (format "- [[%s/%sindex.html][~%s~]]
" b-url u u))) (split-string projects))
#+end_src

*** Bucket Index
    :PROPERTIES:
    :custom_id: project-index-title
    :export_file_name: index.html
    :export_subtitle: {{{version}}} created {{{upload-date}}}
    :END:
#+html_doctype: html5
#+options: toc:nil html5-fancy:t

#+html: <hr>

**** Links for bucket call_bucket-name()
     :PROPERTIES:
     :unnumbered: t
     :custom_id: project-index-links
     :END:

#+call: bucket-project-links()
** Project Readme
This adds the README.md template to a project. It should be customized uniquely
for the project.

#+name:project-readme
#+header: :tangle README.md
#+begin_src markdown
# TITLE
## Subtitle
## Author
## Date
## Version
# ABSTRACT
This is the Org Template file.	It is the parent of all other Org Info blogs,
and provides the source code for processing them in various different ways.
# INTRODUCTION
# CHAPTER
## Section
### Subsection
#+end_src

** Boot Template
:PROPERTIES:
:dependency1: EMACS:=:/Applications/MacPorts/Emacs.app/Contents/MacOS/Emacs or similar
:dependency2: EDITOR:=:emacsclient
:dependency3: =SYNC_ORG_TEMPLATE= defined as $DEV/Templates/Org/Template.org
:END:
Although running the command ~org-babel-tangle~ (=C-c C-v t=) from within Emacs
will install  everything, it would  be nice to have  a simple Makefile  that is
downloaded with this  file that could be  invoked to do the  same thing without
starting Emacs and Org-mode and keying in the ~org-babel-tangle~ command.  This
little Makefile should be stored on  GitHub along with the ~Template.org~ file.
When  the source  is extracted  to a  directory, then  running this  Makefile's
default rule  as simply ~make~  will extract the ~preprocess.el~  script, which
updates  =DEV= and  then  extracts the  full Makefile.   Because  this file  is
tangled along with the full Makefile, it simply gets tacked onto the end of the
big Makefile as an additional rule.   Now, running ~make~ runs the default rule
from the  main Makefile, which is  to extract everything, then  export to TEXI,
INFO, HTML, and PDF forms.

It is assumed that an Emacs server is running, and that the $EDITOR environment
variable is set to use ~emacsclient~.

#+name:boot-template
#+header: :tangle Makefile
#+begin_src makefile
  boot:
	  $(EDITOR) -u --eval \
		  "(with-current-buffer (car (find-file-noselect \"./*.org\" nil nil t)) \
			  (goto-char (point-min)) \
			  (re-search-forward \"^#[+]name:preprocess.el$$\") \
			  (org-babel-tangle (quote (4))) \
			  (save-buffer) \
			  (kill-buffer))" \
	  --eval \
		  "(let ((rsrcdir \"resources\") \
			 (subdirs (list \"tools\" \"images\"))) \
		     (mkdir rsrcdir t) \
		     (dolist (subdir subdirs) (mkdir (concat rsrcdir \"/\" subdir) t)))"
	  ./resources/tools/preprocess.el
#+end_src

** Preprocess Env Vars
The environment variable DEV can be  in different locations and will be spelled
differently based  on how the  local machine is set  up.  For instance,  on one
system,  it will  be at  ~$HOME/Dev~  while in  another  system it  will be  at
~/usr/local/dev~.  However, the =:tangle= keyword  does not expand variables in
the form ~${DEV}~,  but rather requires absolute  paths, like ~/usr/local/dev~.
Therefore, this program works like a preprocessor for environment variables set
up  as part  of  =:tangle= lines,  changing them  to  their system  environment
variable values prior to tangling.  It lives in the ~resources/tools~ directory.

#+name:preprocess.el
#+header: :mkdirp t
#+header: :tangle resources/tools/preprocess.el
#+header: :shebang "#!/opt/local/bin/emacs -Q --script"
#+begin_src emacs-lisp
  (with-current-buffer (car (find-file-noselect "./*.org" nil nil t))
    (save-excursion
    (goto-char (point-min))
    (let ((re-search-str "\\(?::tangle\\|load-file \\(?:[\\]*\\)?[\"]\\)\s*\\(.*?/[dD]ev\\)/")
          (dev (getenv "DEV")))
      (while
              (re-search-forward re-search-str nil t)
              (replace-match dev t nil nil 1)))
    (save-buffer)
    (require 'org)
    (org-babel-tangle)))
#+end_src

** Samples
#+begin_comment
(cd "~/Dev/Emacs/MasteringEmacs/")
"/Users/pine/Dev/Emacs/MasteringEmacs/"

(defun add-bucket (org bucket)
  "Add a bucket keyword BUCKET to the org file ORG."
  (interactive "fFile: \nsBUCKET: ")
  (with-current-buffer (find-file-noselect org)
    (let* ((tree (org-element-parse-buffer))
	   (ins (car (org-element-map tree (quote section)
		 (lambda (s)
		   (org-element-map s (quote keyword)
		     (lambda (kw) (when (equal "MACRO" (org-element-property :key kw)) (1- (org-element-property :end kw))))
		     nil nil :keyword))
		 nil t nil nil))))
      (goto-char ins)
      (insert (format "#+bucket:%s\n" bucket))
      ())))

(add-bucket "MasteringEmacs.org" "pinecone-forest")
nil

(defun hl-region (raw-hl)
  "Obtain the begin and end positions for a headline."
  (with-current-buffer (find-file-noselect (getenv "SYNC_ORG_TEMPLATE"))
    (let* ((tree (get-parsed-tree))
	   (hl (car-safe (org-element-map tree 'headline
			   (lambda (hl) (when
					    (string= raw-hl
						     (org-element-property :raw-value hl))
					  (org-element-context)))
			   nil nil t))))
      (cons
       (org-element-property :begin hl)
       (org-element-property :end hl))
      )))

(hl-region "Build Tools")

(4888 . 29646)

(defun get-hl-with-prop (org-dir hl-prop)
  "Given a directory containing an Org template file and a custom_id property name, return the headline containing that custom_id, or nil if none."
  (progn
    (cd org-dir)
    (let ((org-buf (car-safe (find-file-noselect "*.org" nil nil t))))
      (if org-buf
	  (with-current-buffer org-buf
	    (let ((tree (org-element-parse-buffer)))
	      (org-element-map tree 'headline
		(lambda (hl)
		  (let ((cid (org-element-property :CUSTOM_ID hl)))
		    (when (string= hl-prop cid)
		      (and
		       (message (format "Found the headline %s containing property %s." (org-element-property :raw-value hl) hl-prop))
		       hl))))
		nil t)))
	(and
	 (message (format "The directory %s does not contain an Org file." org-dir))
	 nil)))))

(get-hl-with-prop "~/Dev/Templates/Org" "build-tools")

(headline (:raw-value "Build Tools" :begin 4888 :end 29646 :pre-blank 0 :contents-begin 4902 :contents-end 29645 :level 1 :priority nil :tags nil :todo-keyword nil :todo-type nil :post-blank 1 :footnote-section-p nil :archivedp nil :commentedp nil :post-affiliated 4888 :FROM-FILE "Template" :CUSTOM_ID "build-tools" :APPENDIX "t" :title "Build Tools"))









;;; Add a keyword named 'bucket' just after the version macro.
;;; This function should be run from within the directory containing the Org file.
(defun add-bucket (org-file s3-bucket)
  "Add the name of the associated AWS S3 bucket to an Org templated file."
  (with-current-buffer (find-file-noselect org-file)
    (goto-char (point-min))
    (let* ((tree (org-element-parse-buffer))
	   ;; find the beginning position of the first headline to act as a limit
	   (hl1 (org-element-map tree (quote headline) (lambda (hl) (org-element-property :begin hl)) nil t)))
      ;; Check for the presence of a bucket keyword before the first headline
      (unless (re-search-forward "^#\\+bucket:" hl1 t)
	;; If no bucket keyword is found, search for a keyword MACRO with the value 'version'
	(org-element-map tree (quote keyword)
	  (lambda (kw) (when (and (string= "MACRO" (org-element-property :key kw))
				  (string-match-p "version" (org-element-property :value kw)))
			 ;; return the end position of the MACRO; subtract an empty line if there is one
			 (goto-char (- (org-element-property :end kw) (org-element-property :post-blank kw)))
			 (insert "#+bucket:" s3-bucket)
			 (newline)
			 (basic-save-buffer)
			 (message (format "Added bucket %s" s3-bucket))))
	  nil t)))))

(add-bucket "MasteringEmacs.org" "pinecone-forest.com")
nil

"Added bucket pinecone-forest.com"









(keyword (:key "MACRO" :value "version Version 0.0.108" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...))
("TITLE" "SUBTITLE" "AUTHOR" "DATE" "MACRO" "TEXINFO" "TEXINFO" "CINDEX" "CINDEX" "CINDEX" "CINDEX" "CINDEX" ...)







((keyword (:key "MACRO" :value "version Version 0.0.107" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...)))
#+end_comment

* Build Scripts
  :PROPERTIES:
  :custom_id: build-scripts
  :END:
** Create and Update Projects
*** Create New Project
 Copy this project template file into a new directory, update its title, author,
 and AWS S3 bucket, and tangle the bootstrap Makefile and initial Readme, then
 create a new git repository and create an initial git commit. Finally, save the
 project in GitHub, and there add a description and link to its AWS S3 bucket.
**** DONE List of todos [1/1]
     - State "DONE"       from "TODO"       [2020-11-07 Sat 16:53]
     - State "TODO"       from              [2020-11-07 Sat 09:47] \\
       List of todo's for creating a new project
 - [X] Make sure env var SYNC_ORG_TEMPLATE exists or throw an error

**** Create New Project Code
 #+pindex:org-template.el
 #+caption:Create New Project Code
 #+name:create-new-project
 #+header: :tangle /usr/local/dev/bin/org-template.el
 #+begin_src emacs-lisp
   (defun create-new-project (project title author &optional bucket)
     "Create a new project in a new directory.

   - Depends on env var SYNC_ORG_TEMPLATE existing and pointing to this file.
   - Require a title, author, and AWS S3 bucket name
   - Create new directory structure: resources, and subdirs tools, images, etc
   - Copy template file into a new directory.
   - Update its title, author, and S3 bucket entries.
   - Tangle the bootstrap makefile and Readme.
   - Create a new git repo and add an initial commit.
   - Upload the git repo into GitHub, add a description to the GitHub repo."
     (message "NEW PROJECT=%s TITLE=%s AUTHOR=%s BUCKET=%s" project title author bucket)

     (setq SOT (getenv "SYNC_ORG_TEMPLATE"))
     (access-file SOT "Cannot access SYNC_ORG_TEMPLATE")

     ;; Project directory structure
     (setq project-dir (file-name-as-directory project))
     (setq project-file (concat project-dir project ".org"))
     (setq resources-dir "resources/")
     (setq projrsc-dir (concat project-dir resources-dir))
     (setq resources-subdirs (list "tools/" "images/"))

     ;; make the directories
     (mkdir project-dir t)
     (mkdir projrsc-dir)
     (dolist (subdir resources-subdirs)
       (mkdir (concat projrsc-dir subdir)))
     ;; create the project file from the template
     (copy-file SOT project-file)

     (with-current-buffer (find-file-noselect project-file)
       (let* ((cur-buf (current-buffer))
	      (proj-tree (project-tree cur-buf))
	      (delete-hl-list (list "build-scripts" "todos" "README")))

	 ;; Remove some sections
	 (message "Deleting %s..." delete-hl-list)
	 (seq-do (lambda (cid)
		   (message "...Deleting %s in %s..." cid cur-buf)
		   (let* ((beg-end (find-hl-cid proj-tree cid))
			  (beg (car beg-end))
			  (end (cdr beg-end)))
		     (message "   ...%s: %s" cid beg-end)
		     (delete-region beg end)
		     (message "   ...done")))
		 delete-hl-list))
       (message "Done deleting.")

       ;; update title, author, version, bucket
       ;; with values provided by the user
       (goto-char (point-min))
       (re-search-forward "^#[+]title:\s*\\(TITLE\\)$")
       (replace-match title t nil nil 1)
       (re-search-forward "^#[+]author:\s*\\(AUTHOR\\)$")
       (replace-match author t nil nil 1)
       (re-search-forward "^#[+]macro:\s*version Version \\(.*\\)$")
       (replace-match "0.0.0" t nil nil 1)
       (when bucket
	 (re-search-forward "^#[+]bucket:\s*\\(.*\\)$")
	 (replace-match bucket t nil nil 1))
       (re-search-forward "^#[+]texinfo_printed_title:\\(PRINTED TITLE\\)$")
       (replace-match (concat project "---" title) t nil nil 1)
       (save-buffer)

       ;; tangle the project readme and boot makefile
       (goto-char (point-min))
       (org-babel-goto-named-src-block "project-readme")
       (org-babel-tangle (quote (4)))
       (org-babel-goto-named-src-block "boot-template")
       (org-babel-tangle (quote (4)))
       (kill-buffer)))
 #+end_src

*** Update Old Project
**** Add a Key-Value
This script replaces ~add-s3-bucket~ with more abstract code to add any
key-value combo after the =version= macro.  It takes two values, the key and
the value.  It checks whether the key exists in the current Org file as
=#+key:=.  If it does, then it simply returns nil.  If it does not, then it
adds =#+key:value" after the =version= macro.

NOTE: there is already an org procedure called ~org-collect-keywords~ that
searches for and return key-value pairs; consider using it instead.

#+name:add-key-value
#+header: :tangle /usr/local/dev/bin/org-template.el
#+begin_src elisp
    (defun add-key-value (key value)
      "Check for the existence of a key KEY; if not found, add it
    with value VALUE."
      (with-current-buffer (car (find-file-noselect "*.org" nil nil t))
        (goto-char (point-min))
        (let* ((tree (project-tree (current-buffer)))
               ;; hl1 limits the search to the first headline
               (hl1 (org-element-map tree (quote headline) (lambda (hl) (org-element-property :begin hl)) nil t)))
          (message  "In add-key-value; searching for key: %s..." key)
          (if (re-search-forward (format "^#[+]%s:" key) hl1 t)
              (message "...found.")
            (progn
              (message "...key: %s not found; adding value: %s..." key value)
              (re-search-forward "^#[+]macro:version")
              (beginning-of-line 2)
              (insert (format "#+%s:%s" key value))
              (message "Added #+%s:%s at point %d" key value (line-beginning-position))
              (newline)
              (save-buffer)
              (revert-buffer t t t))))))
#+end_src
**** Add a Custom_Id
 This code checks a project file F to see if it contains a =property: value=
 pair (P, V) in a property drawer right under the headline HL. If it does not,
 it adds one. It will return nil if the headline is not found.

 #+name:add-pv-to-hl
 #+header: :tangle /usr/local/dev/bin/org-template.el
 #+begin_src emacs-lisp
   (defun add-pv-to-hl (f hl p v)
     "In file F add a property P with value V into a property
   drawer (creating one if necessary) at headline HL."
     (message "In `add-pv-to-hl' with file:`%s' headline:`%s' property:`%s' value:`%s'" f hl p v)
     (with-current-buffer (find-file-noselect f)
       (save-excursion
	 (goto-char (point-min))
	 (let ((found (re-search-forward (concat "^[*]\\{1,\\}\s*" hl))))
	   (message "...found %s" hl)
	   (beginning-of-line 2)
	   (let* ((e (org-element-at-point))
		  (et (org-element-type e)))
	     (unless (string= et "property-drawer")
	       (message "...adding property drawer to headline:`%s'" hl)
	       (org-insert-property-drawer))
	     (unless (org-entry-get (point) p)
	       (message "...adding property:`%s' with value:`%s' to property drawer." p v)
	       (org-entry-put (point) p v))))
	 (when (buffer-modified-p)
	   (message "Saving buffer:`%s'" (current-buffer))
	   (save-buffer)))))
 #+end_src

**** Replace Build Tools
These    three   small    scripts    are    used   by    ~replace-build-tools~.
~org-template-version~  returns   the  main  Org  Template's   version  number.
~project-tree~ returns  a parsed  tree from a  buffer. ~find-hl-cid~  locates a
level 1 heading that contains a particular custom id and returns that subtree's
beginning and ending points.

#+name: replace-utilities
#+header: :tangle /usr/local/dev/bin/org-template.el
#+begin_src emacs-lisp

  (defun org-template-key-value (key &optional regexp-str)
    "Given a KEY string and an optional REGEXP-STR string, in the file
  SYNC_ORG_TEMPLATE find the key and return the value of the match
  string, which defaults to (.*)$ if nil."
    (with-current-buffer (find-file-noselect (getenv "SYNC_ORG_TEMPLATE"))
      (save-excursion
	(let ((regexp-use-str
		(or regexp-str
		    "\\(.*\\)$")))
	  (goto-char (point-min))
	  (re-search-forward (concat "^#[+]" key regexp-use-str))
	  (match-string-no-properties 1)))))

  (defun org-template-version ()
    "Return the current version number of SYNC_ORG_TEMPLATE."
    (org-template-key-value "macro:\s*version Version " "\\(\\(?:[[:digit:]]+[.]?\\)\\{3\\}\\)"))

  (defun org-template-bucket ()
    "Return the bucket name of SYNC_ORG_TEMPLATE."
    (org-template-key-value "bucket:\s*"))


  (defun project-tree (proj-buf)
    "With a buffer PROJ-BUF, return an Org-parsed tree"
    (with-current-buffer proj-buf
      (org-element-parse-buffer 'headline)))


  (defun find-hl (buf hl)
    "With a buffer BUF, find a headline HL.

  Return nil if the headline is not found. Return its point if it
  is found."
    (with-current-buffer buf
      (save-excursion
	(goto-char (point-min))
	(let ((re (format "^[*]\\{1,\\}\\s\\{1,\\}%s$" hl)))
	  (re-search-forward re nil t)))))

  (defun find-hl-cid (proj-tree cid)
    "With an Org-parsed tree PROJ-TREE,  find a headline of leval 1
  or 2 with a particular property drawer custom_id of CID."
    (let* ((cid-hl (org-element-map proj-tree 'headline
	    (lambda (e) (let ((lev (org-element-property :level e))
			      (bt (org-element-property :CUSTOM_ID e)))
			  (and (<= lev 2)
			       (string= bt cid)
			       e)))
	    nil t))
	   (car cid-hl))
      (cons (org-element-property :begin cid-hl)
	    (org-element-property :end cid-hl))))

#+end_src

The ~replace-build-tools~ function replaces a section of an old templated file
with the corresponding section from the source template file found in
=SYNC_ORG_TEMPLATE=, which is presumably newer. It must be run from within the
directory holding the older templated file, and the original template file must
be identified by the environment variable =SYNC_ORG_TEMPLATE=.

This function works by parsing the buffers by headlines and then using
~org-element-map~ to find a level 1 headline containing a custom id of CID.
Once it finds such a headline, it records that section's beginning and ending
points. It does this for both the old template file and the template file. Then
it deletes that section from the old template file, and inserts the
corresponding section from the root template file.

#+name: replace-build-tools
#+header: :tangle /usr/local/dev/bin/org-template.el
#+begin_src emacs-lisp

  (defun replace-build-tools (cid)
    "Replaces a section of an Org template file identified by the
  custom_id CID with the corresponding section of the root
  template identified by the environment variable.

  OLD-BUF is the Org template in the current working directory.
  SYNC_ORG_TEMPLATE must be set."
    ;(message "...in replace-build-tools with %s..." cid)
    (let* ((old-buf (car (find-file-noselect "./*.org" nil nil t)))
	   (sync-buf (find-file-noselect (getenv "SYNC_ORG_TEMPLATE")))
	   (old-tree (project-tree old-buf))
	   (sync-tree (project-tree sync-buf))
	   (old-be (find-hl-cid old-tree cid))
	   (sync-be (find-hl-cid sync-tree cid)))
	(set-buffer old-buf)
	(delete-region (car old-be) (cdr old-be))
	(goto-char (car old-be))
	(insert-buffer-substring sync-buf (car sync-be) (cdr sync-be))
	(goto-char (car old-be))
	(org-set-property "org-template-version" (org-template-version))
	(save-buffer)))

  (defun delete-build-tools (cid)
    "Delete a section of an Org template file identified by the
  custom_id CID."
    ;(message "...in delete-build-tools at %s..." cid)
    (let* ((old-buf (car (find-file-noselect "./*.org" nil nil t)))
	   (old-tree (project-tree old-buf))
	   (old-be (find-hl-cid old-tree cid)))
      ;(message "old-buf: %s\told-be: %s" old-buf old-be)
      (with-current-buffer old-buf
	(delete-region (car old-be) (cdr old-be))
	(save-buffer))))
#+end_src

**** Replace Build Tools Script
Add a script in ~bin~ to run just the ~replace-build-tools~, then tangle the
~Makefile~.

#+name:replace-build-tools-script
#+header: :tangle /usr/local/dev/bin/replace-build-tools
#+header: :shebang "#! /Applications/MacPorts/Emacs.app/Contents/MacOS/Emacs --script"
#+begin_src elisp
(load-file "/usr/local/dev/bin/org-template.el")
(replace-build-tools "build-tools")
(kill-buffer)
#+end_src

*** Run Create and Update
**** Run Create
 This shell script  is installed into $DEV/bin  and is run by  typing the shell
 command =new-org-template <...ARGS>= from the command-line to set up a new Org
 project at a  particular point in your directory structure.   The main code is
 elisp, described above in [[*Create New Project][Create New Project]].

 After a new project  is installed into the directory structure,  a git repo is
 established, as well  as a new GitHub  repo, and the initial  commit is pushed
 up.

 #+name:new-org-template
 #+header: :tangle /usr/local/dev/bin/new-org-template
 #+header: :shebang "#!/usr/bin/env zsh"
 #+begin_src sh -n
   # new-org-template
   # $1 := project
   # $2 := title
   # $3 := author
   # [$4 := bucket] (default := ${AWS_S3_BUCKET})

   USAGE="$0 <project> <title> <author> [<bucket>]\n"
   [[ -z $AWS_S3_BUCKET ]] && {
       # printf goes to standard out by default; redirect this error message to standard error
       printf "${RED}ERROR: ${YELLOW}The environment variable ${GREEN}\$AWS_S3_BUCKET${YELLOW} needs to be set.${CLEAR}\n" >&2
       exit 1
   }

   if [[ $1 =~ ^-(h|-?help)$ || ( $# < 3 || $# > 4 ) ]]; then
       printf "USAGE:\n$USAGE"
       exit 0;
   fi

   # verify command-line args contain only letters, digits, underscores, dashes and spaces
   RE="^[_a-zA-Z][_a-zA-Z0-9 -.]+$"
   for arg in "$@"; do
       printf "$arg..."
       [[ $arg =~ $RE ]] || { printf ": ERROR\n"; exit 1; }
       printf "ok\n"
   done

   # bucket is optional;
   # if it is supplied, make sure it has a suffix, e.g. '.com' or '.org'
   # if not, add '.com' as the default;
   # let this be known on standard error
   bucket=${4:-${AWS_S3_BUCKET}}
   [[ ${bucket} == ${bucket%.*} ]] && {
      bucket=${bucket}.com
      # printf goes to standard out by default; redirect this error message to standard error
      printf "${YELLOW}The bucket name has had the suffix ${BLUE}.com${YELLOW} added: ${PURPLE}${bucket}${CLEAR}" >&2
   }

   ${EDITOR} --eval "(progn
			 (load-file \"/usr/local/dev/bin/org-template.el\")
			 (create-new-project \"$1\" \"$2\" \"$3\" \"$bucket\"))"

   # create a new Git repo and GitHub repo
   cd "$1"
   rm *~
   git init
   git add -A
   git commit -m "Initial commit"
   git log | cat
   git remote add origin git@github.com:wlharvey4/"$1".git

   curl -i -H "Authorization: token ${GITHUB_TOKEN}" \
	-d "{\"name\":\"$1\",\"description\":\"$2: v0.0.0/\"}" \
	https://api.github.com/user/repos

   git push origin master

   cd ..
   tree -a -L 1 "$1"
 #+end_src

**** Run Update
The command-line utility  ~update-org-template~ is run from  within a directory
containing  an out-dated  Org template  project. It  deletes the  file's "Build
Tools" and "Buid  Scripts" subtrees and replaces them with  those from the main
template file.

 #+header: :tangle /usr/local/dev/bin/update-org-template
 #+header: :shebang "#!/Applications/MacPorts/Emacs.app/Contents/MacOS/Emacs --script"
 #+begin_src emacs-lisp
   (require 'org)
   (message "Update Org Template...")
   ;(message "Obtaining ORG_SYNC_TEMPLATE: %S..." (getenv "SYNC_ORG_TEMPLATE"))
   (org-babel-tangle-file (getenv "SYNC_ORG_TEMPLATE"))
   ;(message "obtained.")

   (load-file "/usr/local/dev/bin/org-template.el")

   (setq old-template (car (file-expand-wildcards "*.org" t)))
   ;(message (format "loaded old-template: %s" old-template))
   (setq org-template-version (org-template-version))
   ;(message (format "org-template-version: %s" org-template-version))

   (message "Checking for s3-bucket: %s..." (org-template-bucket))
   (add-key-value "bucket" (org-template-bucket))
   ;(add-s3-bucket (org-template-bucket))

   (message "Checking for `macro upload-date'")
   (add-key-value "macro" "upload-date (eval (current-time-string))")
   ;(message "done")

   ;(message "Adding pv to build-tools...")
   (add-pv-to-hl old-template "Build Tools" "custom_id" "build-tools")
   ;(message "done.")

   ;(message "Replacing build-tools...")
   (replace-build-tools "build-tools")
   ;(message "done.")

   ;(message "Adding template version to Makefile...")
   (add-pv-to-hl old-template "Makefile" "org-template-version" org-template-version)
   ;(message "done.")

   ;(message "Adding pb build-scripts...")
   (when (find-hl (get-file-buffer old-template) "Build Scripts")
     (add-pv-to-hl old-template "Build Scripts" "custom_id" "build-scripts")
     ;(message "done.")

     ;(message "Deleting build-scripts...")
     (delete-build-tools "build-scripts"))
     ;(message "done.")

   (org-babel-tangle-file old-template)
   (message "Finished Update Org Template.")
 #+end_src

** Ignore
  #+name:update-org-project
  #+begin_src emacs-lisp
    (defun get-parsed-tree (buf)
      "Given a buffer BUF, return a parsed tree."
      (with-current-buffer buf
	(org-element-parse-buffer 'headline)))

    (defun oep (p e)
      "Given a property P and element E, return that property's value."
      (org-element-property p e))

    (defun start-end (buf prop)
      "Given a buffer BUF and a property PROP, return the starting and ending points."
      (org-element-map
	  (get-parsed-tree buf)
	  'headline
	  (lambda (e) (when (string= (oep :CUSTOM_ID e) prop)
			(list
			 (oep :begin e)
			 (oep :end e))))
	  nil t))

    (defun update-old-project (&optional title author bucket)
      "Update an old project with new code."
      (with-current-buffer (car (find-file-noselect "./*.org" nil nil t))
	(save-excursion
	(goto-char (point-min))
	;; (when title
	;;   (re-search-forward "^#[+]title:\s*\\(.*\\)$")
	;;   (replace-match title t nil nil 1))
	;; (when author
	;;   (re-search-forward "^#[+]author:\s*\\(.*\\)$")
	;;   (replace-match author t nil nil 1))
	;; (when bucket
	;;   (re-search-forward "^#[+]bucket:\s*\\(.*\\)$")
	;;   (replace-match bucket t nil nil 1))
	(let* ((curbuf (current-buffer))
	      (orgsyncbuf (find-file-noselect (expand-file-name "Template.org" "/usr/local/dev/Templates/Org")))
	      (start-end-curbuf (start-end curbuf "build-tools"))
	      (start-end-sync (start-end orgsyncbuf "build-tools")))
	  (list start-end-curbuf start-end-sync)))))
  #+end_src

  #+begin_src emacs-lisp :results raw
  (update-old-project)
  #+end_src

  #+RESULTS:
  ((4928 36833) (4928 36833))

*** Create and Update Script                                   :dependencies:
    :PROPERTIES:
    :dependency1: org-template must be in $DEV/bin
    :dependency2: SYNC_ORG_TEMPLATE must be set as an environment variable
    :dependency3: "COLORS from profile"
    :dependency4: tree command
    :dependency5: git command
    :END:
 This code  is a script file  to create a  new project from this  template, and
 also  to  update a  project  with  updated scripts.  It  is  tangled into  the
 ~$DEV/bin~  directory and  is called  from the  command line  as ~org-template
 <project> [<author>]  [git]~ or  as ~org-template -u  | --update~.  Its create
 mode  takes one  required,  and up  to two  optional  arguments. The  required
 argument is the name of the project.  One optional argument is the name of the
 author. The other optional argument is the term =git=, meaning to initialize a
 =git=  repository for  the project.  To update  a project,  call ~org-template
 --update~ from the project root.

 Here are the steps it takes:

 1. It checks for a call to =-h|--help=, and if found, prints the USAGE message.

 2. It next checks for =-u|--update=, and if found, calls the ~template-update~
    function. Otherwise, it calls the ~template-create~ function.

 3. It creates a new directory in the current working directory using the
    =project= argument.

 4. It copies this template into it (using the environment variable
    =SYNC_ORG_TEMPLATE= to find it) as a new Org file using, again, the name of
    the project.

 5. It then updates the title to the project name, the date, sets the version
    number to =0.0.0=, and optionally adds the author, using the =author=
    argument if it was given.

 6. It then deletes this script from the new Org project file, as it is not
    needed by a project file.

 7. It then tangles Boot Makefile and the default ~README.md~ into the project.

 8. If the term ='git'= is supplied as an argument, it initializes a new Git
    repository, creating a basic ~.gitignore~ file in it, adding the Org file
    and the ~README.md~ file and finally making an initial Git commit.

 9. Last, it prints an outline of the project’s structure using the ~tree~
    command.


 #+caption:Create Script
 #+name:create-update-script
 #+header: :mkdirp yes
 ##+header: :tangle /usr/local/dev/bin/org-template
 #+header: :shebang "#!/usr/bin/env bash"
 #+begin_src sh -n
   # org-template: create and update projects

   USAGE='
   '"$0..."'
   org-template -h | --help
   org-template <Project> [<Author>] [git]
   org-template -u | --update
   '

   template-create () {
       mkdir -v "$1"
       printf "copy "
       cp -v ${SYNC_ORG_TEMPLATE} "$1/$1.org"
       printf "${CLEAR}\n"

       sed -i '' -Ee '/^\#\+(title|TITLE):/ s/TITLE/'"$1"'/' \
	   -Ee '/^\#\+(date|DATE):\s*(.*)$/ s/$2/$(date '"+%F %R"')/' \
	   -Ee '/^\#\+(macro|MACRO):version Version/ s/[[:digit:].]+/0.0.0/' \
	   -Ee '/^\#\+(texinfo_printed_title|TEXINFO_PRINTED_TITLE):/ s/PRINTED TITLE/'"$1"'/' \
	   "$1/$1.org"

       [[ $# -ge 2 ]] && \
	   sed -i '' -Ee '/^\#\+(author|AUTHOR):/ s/AUTHOR/'"$2"'/' "$1/$1.org"

       printf "${CYAN}"
       ${EDITOR} --eval \
       "(with-current-buffer (find-file-noselect \"$1/$1.org\")
	  (save-excursion
	   (goto-char (point-min))
	   (re-search-forward \"#[+]name:project-readme$\")
	   (org-babel-tangle (quote (4)))
	   (search-forward \"** Create Script\")
	   (org-cut-subtree)
	   (save-buffer)
	   (re-search-forward \"^#[+]name:boot-template$\")
	   (org-babel-tangle (quote (4)))))"
       printf "${CLEAR}\n"

       if [[ (($# -eq 2) || ($# -eq 3)) && (($2 == 'git') || ($3 == 'git')) ]]
       then
	   cd $1 && {
	       rm *~
	       printf "${YELLOWBOLD}"
	       git init
	       printf "${CLEAR}"

	       echo "\
   .gitignore
   Makefile
   ,*~
   .*~
   ,*.texi
   ,*.info
   ,*.html
   ,*-old
   tools" > .gitignore

	       git add .
	       git commit -m "Initial commit of Project $1"
	   }
	   cd ..
       fi
       rm "$1"/*~

       printf "${PURPLEBOLD}\n"
       pwd
       printf "${CLEAR}"
       tree -aI .git $1

       return 0
   }

   template-update() {
       ${EDITOR} -u --eval \
		"(progn
		  (with-current-buffer (find-file-noselect (getenv \"SYNC_ORG_TEMPLATE\"))
		   (save-excursion
		    (goto-char (point-min))
		    (org-link-search \"#build-tools\")
		    (org-copy-subtree 2)))
		  (with-current-buffer (car (find-file-noselect \"./*.org\" nil nil t))
		   (save-excursion
		     (goto-char (point-min))
		     (org-link-search \"#build-tools\")
		     (org-paste-subtree 1 nil t t)
		     (org-cut-subtree 2)
		     (save-buffer))))"

       return 0;
   }

   main () {
       case $1 in
	   create)

	       printf "${PURPLE}"
	       read -n 1 -p "Create new project: $2 (y/n) ?"
	       printf "${CLEAR}\n\n"
	       [[ $REPLY =~ [yY] ]] && template-create "$2" || exit 0
	       ;;

	   update)

	       template-update
	       ;;
       esac
       exit 0
   }


   if [[ $1 =~ ^-(h|-help)$ ]]; then
	   printf "HELP:\n$USAGE"
	   exit 0
   fi

   if [[ $1 =~ ^-(u|-update)$ ]]; then
	   printf "UPDATE:\n"
	   main update
   fi

   [[ ($# -ge 1) && ($# -le 3) ]] && { printf "CREATE:\n";  main create $1; } || {
       printf "${RED}ARGUMENTS ERROR:${CLEAR}$USAGE\n"
       exit 1
   }

 #+end_src
** Switch Emacs Init
This script allows the user to switch into using a different Emacs
initialization setup.  The script first lists the currently-selected
initialization setup, then it lists the available initialization setups, then
requests the user's choice.  After obtaining the choice, it changes the
symbolic link in =~/.emacs.d= to that chosen by the user.  Emacs is then killed
and restarted using the ~desktop-save~ feature.

Each initialization setup is a complete =~/.emacs.d= subtree, which must be set
up by the user, with its name given after a dash, such as =~/.emacs.d-original=
or =~/.emacs.d-cfbt= (“Clojure for the Brave and True”).

#+name:switch-emacs-init
#+header: :mkdirp yes
#+header: :shebang "#!/usr/bin/env bash"
#+header: :tangle /usr/local/dev/bin/switch-emacs-init
#+begin_src sh
  printf "${GREEN}"
  ls -l ~/.emacs.d | cut -f 12- -d ' '
  printf "${CLEAR}"
  echo
  select choice in $(ls -1d ~/.emacs.d-*) "abort"
  do
      echo -n 'You chose '
      printf " ${B_YELLOW}${F_BLACK}$choice${CLEAR}  "
      [[ $choice = "abort" ]] && exit 0
      rm ~/.emacs.d
      printf "${CYAN}"
      ln -vs $choice ~/.emacs.d
      echo
      printf "${RED}"
      read -N 1 -p "Restart Emacs now? (y/n) "
      printf "${CLEAR}\n"
      [[ $REPLY =~ y|Y ]] || { echo "Not restarting"; break; }
      echo "Restarting..."
      emacsclient --eval '(progn (desktop-save "~/.emacs.d-original/")(kill-emacs))'
      break
  done
  /Applications/MacPorts/EmacsMac.app/Contents/MacOS/Emacs --eval '(progn (server-start)(desktop-read "~/.emacs.d-original/"))' &
#+end_src

** Update RC Files
This code synchronizes the following configuration files:
- /etc/profile
- /etc/bashrc
- /etc/tmux.conf
- ~/.bash_profile
- ~/.bashrc
- ~/.tmux.conf


It first verifies that the files differ; then it will copy a newer file over an
older file.  If a  local file is updated with a newer  template file, then this
program updates the  Emacs ~server-socket-dir~ variable, if such  exists.  If a
template   is	updated	  with	 a   newer  local   file,   then   delete   the
~server-socket-dir~ value.

#+name:syncrc
#+header: :mkdirp yes
#+header: :tangle /usr/local/dev/bin/syncrc
#+header: :shebang "#!/usr/bin/env bash"
#+begin_src shell
  [ -v TEMPLATES ] || {
      printf "${RED}ERROR: missing \$TEMPLATES env var${CLEAR}\n"
      exit 1
  }

  [ -v COMP ] || {
      printf "${RED}ERROR: missing \$COMP env var${CLEAR}\n"
      exit 1
  }

  set -e

  ## usage: syncrc [-f system|rc -t rc|system -h]
  ## if no options, process by natural age
  ## if -f and -t options, force update from ... to ...
  usage () {
	printf "\n${GREEN}USAGE: ${YELLOW}syncrc \
  ${WHITEBOLD}[${CYAN}-f ${MAGENTA}system|rc \
  ${CYAN}-t ${MAGENTA}rc|system \
  ${WHITEBOLD}]${CLEAR}\n"

	printf "${GREEN}force sync rc files \
  ${CYAN}'-f'${GREEN}rom ${MAGENTA}system|rc \
  ${CYAN}'-t'${GREEN}o ${MAGENTA}rc|system\
  ${CLEAR}\n\n"
	exit $1
  }

  # force update of either system or rc file
  touchup () {
    printf "In touchup with \$from: $from\n";
	case $from in
	    "system") printf "${WHITEBOLD}Touching $1..."; sudo touch $1; printf "${CLEAR}" ;;
	    "rc")     printf "${WHITEBOLD}Touching $2..."; touch $2; printf "${CLEAR}" ;;
	esac
  }

  # compare files to determine which one is newer
  comp () {
	[[ -e $1 && -e $2 ]] && { # only process if both files exist
	if ! cmp -s $1 $2
	then
	    [[ -n $from ]] && { # force update if $from is non-zero length
		touchup $1 $2
	    } || { :; }
	    [[ $1 -nt $2 ]] && { # system file is newer
		[[ -d $(dirname "$2") ]] || { # make sure rc dir exists
		    mkdir -vp $(dirname "$2")
		} || { :; }

		printf "${YELLOW}system is newer than rc${CLEAR}\n"
		echo; ls -l $1 $2; echo
			  printf "${BLUE}";
			  cp -ipv "$1" "$2";
			  printf "${CLEAR}"

			  ## delete the value of the emacs server-socket-dir in the rc file
			  grep -q "EMACS_SERVER_SOCKET_DIR=" "$2" && {
				  #printf "${WHITE}UNSETTING socket_dir...${CLEAR}\n"
				  sed -i'.bak' -Ee '/(EMACS_SERVER_SOCKET_DIR)=.*$/ s!!\1=TBD!' "$2"
			  } || { :; }

		  } || {
		    [[ "$2" -nt "$1" ]] && { # rc file is newer
		      printf "${YELLOW}rc is newer than system${CLEAR}\n"
		      echo; ls -l $2 $1; echo
		      [[ -w "$1" ]] && { # check if the system file is writable
			printf "${PURPLE}"
			cp -ipv "$2" "$1"
			printf "${CLEAR}"
		      } || {
			printf "${RED}You must authenticate... ${CLEAR}"
			printf "${PURPLE}"
			sudo cp -ipv "$2" "$1"
			printf "${CLEAR}"
		      }

		      ## update the value of the emacs server-socket-dir in the system file
		      grep -q "EMACS_SERVER_SOCKET_DIR=" "$1" && {
			socket_dir=$(${EMACS} -Q --batch --eval '(progn (require (quote server))(princ (file-name-as-directory server-socket-dir)))')
			printf "${WHITE}SETTING socket_dir=${socket_dir}${CLEAR}...\n"
			sed -i'.bak'-$$ -Ee '/(EMACS_SERVER_SOCKET_DIR)=.*$/ s!!\1='"$socket_dir"'!' "$1"
		      } || { :; }

		      } || {
			printf "${REDBOLD}ERROR: the files don't match but are the same age?${CLEAR}\n"
		      }
	      }
	else
	      printf "${CYAN}No difference.${CLEAR}\n"
	fi
	} || {
	    tocreate=$( if test -e "$1"; then echo "$2"; else echo "$1"; fi; )
	    printf "${WHITE}one file: ${RED}${tocreate} ${WHITE}does not exist...Create?${CLEAR}"
	    read -sn 1
	    if [[ $REPLY == [yY] ]]
	    then
		echo " Creating"
		outof=$( if test -e "$1"; then echo $1; else echo "$2"; fi; )
		mkdir -pv $(dirname "${tocreate}")
		cp -ivp "${outof}" "${tocreate}"
	    else
		echo " Not creating"
	    fi
	}
    } # end of comp()

    declare -a rcfiles=(/private/etc/profile /private/etc/bashrc /private/etc/tmux.conf ~/.bash_profile ~/.bashrc ~/.tmux.conf)

    ## see usage() abaove
    while getopts "f:t:h" opt
    do
	  case $opt in
	    'f') from=${OPTARG} ;;
	    't') to=${OPTARG}	;;
	    'h') usage 0	;;
	    '?') printf "${RED}ERROR ${CLEAR}\n"; usage 1; ;;
	     ,*) printf "OPTIND: ${RED}${OPTIND}${CLEAR}\n"; usage 1; ;;
	  esac
    done

    ## OPTIND must be either 1 (no options) or 5 (2 options)
    if [[ $OPTIND -gt 1 && $OPTIND -ne 5 ]]
    then
	  printf "${RED}ERROR: need both -f and -t${CLEAR}\n"
	  usage 1
    fi

    ## check for correct combination of from and to
    if [[ -n $from ]]
    then
	  if [[ $from == "system" ]]
	  then
	    if [[ $to != "rc" ]]
	    then
		printf "${RED}ERROR: incorrect combination: $from -- $to${CLEAR}\n"
		usage 1
	    fi
	  elif [[ $from == "rc" ]]
	  then
	    if [[ $to != "system" ]]
	    then
		printf "${RED}ERROR: incorrect combination: $from -- $to${CLEAR}\n"
		usage 1
	    fi
	  else
	    printf "${RED}ERROR; incorrect combination: $from -- $to${CLEAR}\n"
	    usage 1
	  fi
    fi

    ## everything checks out; now process the files
    for file in "${rcfiles[@]}"
    do
	  printf "Considering ${GREEN}$file... ${CLEAR}"

	  case $file in
	      ,*etc?profile)   comp $file $TEMPLATES/rc/etc/${COMP}/profile   ;;
	      ,*etc?bashrc)    comp $file $TEMPLATES/rc/etc/${COMP}/bashrc    ;;
	      ,*etc?tmux.conf) comp $file $TEMPLATES/rc/etc/${COMP}/tmux.conf ;;
	      ,*bash_profile)  comp $file $TEMPLATES/rc/${COMP}/bash_profile  ;;
	      ,*bashrc)	       comp $file $TEMPLATES/rc/${COMP}/bashrc	      ;;
	      ,*tmux?conf)     comp $file $TEMPLATES/rc/${COMP}/tmux.conf     ;;
	  esac
    done
#+end_src

* List of Programs
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Listing

* List of Examples
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Example

* Copying
:PROPERTIES:
:copying:  t
:END:

Copyright \copy 2020 by {{{author}}}

* Concept Index
:PROPERTIES:
:index: cp
:appendix: yes
:END:

* Program Index
:PROPERTIES:
:index: pg
:appendix: yes
:END:

* Function Index
:PROPERTIES:
:index: fn
:appendix: yes
:END:

* Variable Index
:PROPERTIES:
:index: vr
:appendix: yes
:END:


* Configuration							   :noexport:
#+startup:content

#+todo: SOMEDAY(s@) TODO(t@) INPROGRESS(i@) WAIT(w@) | CANCEL(c@) DONE(d!)

#+options: H:4

#+texinfo_class: info
#+texinfo_header:
#+texinfo_post_header:
#+texinfo_dir_category:<DIR CATEGORY>
#+texinfo_dir_title:<DIR TITLE>
#+texinfo_dir_desc:<DIR DESCRIPTION>
#+texinfo_printed_title:ReactJS71---Learning ReactJS17


* Footnotes

[fn:1]In the browser, add =index.text= to the end of the URL to see the source.

[fn:2]Markdown requires the standard Perl library module Digest::MD5.


* Local Variables						   :noexport:
# Local Variables:
# fill-column: 79
# indent-tabs-mode: t
# eval: (auto-fill-mode)
# time-stamp-pattern: "8/^\\#\\+date:%:y-%02m-%02d %02H:%02M$"
# End:
